<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: OpenGL | Seshbot Programs]]></title>
  <link href="http://seshbot.com/blog/categories/opengl/atom.xml" rel="self"/>
  <link href="http://seshbot.com/"/>
  <updated>2015-05-19T21:25:38+08:00</updated>
  <id>http://seshbot.com/</id>
  <author>
    <name><![CDATA[Paul Cechner]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[An Introduction to OpenGL - Getting Started]]></title>
    <link href="http://seshbot.com/blog/2015/05/05/an-introduction-to-opengl-getting-started/"/>
    <updated>2015-05-05T15:40:37+08:00</updated>
    <id>http://seshbot.com/blog/2015/05/05/an-introduction-to-opengl-getting-started</id>
    <content type="html"><![CDATA[<p><em>This article is a culmination of all the little notes I took while learning OpenGL over the last several months. It&rsquo;s mostly stuff that I found difficult to research plus a little summary of the differences between OpenGL versions.</em></p>

<p><em>What a daunting task!</em></p>

<p><em>If you have any recommendations on how this could be more beginner-friendly please tell me.</em></p>

<p><em>Also thanks <a href="http://greggman.com/">Gregg Tavares</a> for pointing out my various errors!</em></p>

<h2>Things I wish I knew when learning OpenGL</h2>

<p>The most important thing a programmer should know before deciding whether to learn OpenGL is that OpenGL is very low level, poorly documented and extremely crufty. This is because it is an API specification and not a product library per-se. It is up to the many various vendors to implement the API spec as best they can.</p>

<p>Surround your pullquote like this {" text to be quoted "}</p>

<p>The next thing to know about modern OpenGL is that these days it does very little legwork for you other than allowing you to run a program on the GPU. You will have to write the GPU shader programs that do everything from transforming your own application data into screen-space coordinates, to calculating the exact colour of every pixel on the screen incorporating lighting and shading algorithms that you implement yourself (fortunately linear algebra makes this stuff a lot simpler than it sounds!) So OpenGL will not do any inherently 3D stuff for you &ndash; most OpenGL commands and types are capable of describing 3D positions, directions and transformations but you have to do the grunt work yourself.</p>

<p>The third immediate concern &ndash; <em>OpenGL does not work out of the box</em>! An annoying truth is that OpenGL realistically requires supporting libraries in order to function, most importantly to create a context within which the rendering operations can work. It is very common to incorporating at least three libraries &ndash; one to generate a GL context into which you render, a matrix and vector manipulation library, and an extension loader for when you need a little more functionality than your platform provides.</p>

<p>Surround your pullquote like this {" text to be quoted "}</p>

<p>If you are only targeting Windows you might consider DirectX. If you don&rsquo;t need to interact directly with your shaders, and are happy to work at a higher level of abstraction and not with the GPU directly, perhaps a higher level graphics library such as Unity or UDK would work better for you.</p>

<p>So assuming you still want to start using OpenGL, this article might be helpful to you. My intention is to mention a lot of stuff I had to hunt around for that seemed pretty important to me while I was trying to learn it myself. I will not be doing a step-by-step guide to performing specific OpenGL tasks however &ndash; for a good getting started guide check out <a href="https://open.gl/">open.gl</a> which is both modern and easy to follow.</p>

<p>To use OpenGL effectively I figure you&rsquo;d need to understand:</p>

<ul>
<li>opening an OpenGL window (i.e., creating a context)</li>
<li>the basics of rendering:

<ul>
<li>primitives, vertices and fragments</li>
<li>coordinate systems:

<ul>
<li>built-in normalised device coordinates (NDC) and clip coordinates</li>
<li>3D model, view and perspective coordinates</li>
</ul>
</li>
<li>shaders and the render pipeline (how data gets from your app to the screen):

<ul>
<li>vertex and fragment shaders</li>
<li>passing uniforms and attributes into the pipeline</li>
<li>vertex buffers (VBOs)</li>
<li>passing varying data from the vertex shader to the fragment shader</li>
</ul>
</li>
<li>the fixed-function pipeline (now deprecated)</li>
</ul>
</li>
<li>linear algebra (the magical language of graphics programming)</li>
<li>a rundown on all the different major OpenGL versions</li>
<li>major challenges that you certainly will face moving forward</li>
</ul>


<p>I&rsquo;ll leave more advanced core concepts such as framebuffers and textures for a later article.</p>

<!-- more -->


<h2>Starting a new project</h2>

<p>I suppose this is the most important thing to a lot of people, so I&rsquo;ll show how I bootstrap a new OpenGL project. I haven&rsquo;t been at it long so take it with a grain of salt, but I tried to focus on building a cross-platform solution.</p>

<p>I generally depend on four libraries. Although technically you could get away without any supporting libraries, these save a lot of time and effort. The libraries are:</p>

<ul>
<li>Google&rsquo;s <a href="https://code.google.com/p/angleproject/">ANGLE project</a> provides an OpenGL ES 2.0 (soon 3.0) driver library for Windows that wraps Direct3D. This is useful so you don&rsquo;t have to depend on the user downloading the OpenGL drivers for their graphics card on Windows.</li>
<li><a href="glfw.org">GLFW</a> to create a window and otherwise interact with the OS and other devices. Many people prefer <a href="https://www.libsdl.org/">SDL2</a> or <a href="https://www.opengl.org/resources/libraries/glut/">GLUT</a>. Alternatively you could use the standard low-level supporting libraries supported by your operating system &ndash; WGL, GLX or EGL.</li>
<li><a href="glm.g-truc.net">GLM</a> is a widely used vector and matrix library. It&rsquo;s particularly nice because it mirrors GLSL standard types and operations as much as possible, so hopefully there&rsquo;s some learning synergies there. I have tried using <a href="http://eigen.tuxfamily.org/">Eigen</a> which is more of a general linear algebra library focusing on performance, but it has a lot of limitations on how you can use (passing or storing types by value is complicated) it because it uses low-level processor vector operations under the covers. Of course you could always write your own matrix classes, but it&rsquo;s a pretty big task.</li>
<li><a href="http://glew.sourceforge.net/">GLEW</a> is a commonly used extension loader. Unfortunately extension loaders in general don&rsquo;t seem to work with ANGLE so I haven&rsquo;t used it much. <a href="https://github.com/hpicgs/glbinding">glbinding</a> and <a href="https://bitbucket.org/alfonse/glloadgen/wiki/Home">glLoadGen</a> are both code generators that create loaders for specific target versions of OpenGL. These don&rsquo;t seem to be able to target OpenGL ES versions however.</li>
</ul>


<p>I have created a <a href="https://github.com/seshbot/new-gl-app">simple GL application on GitHub</a> that I intended to be used as a starting point for OpenGL ES 2 experimentation. It should work on Windows, Linux and OSX, and the only external dependency should be CMake which is pretty easy to install. Then, hopefully, getting it running is a matter of (depending on your platform of choice):</p>

<p>``` bash
git clone <a href="https://github.com/seshbot/new-gl-app">https://github.com/seshbot/new-gl-app</a>
cd new-gl-app
mkdir build &amp;&amp; cd build
cmake ..</p>

<p>./glapp
```</p>

<p>Alternatively you could try copying <a href="/assets/2015-05-13-gl1.html">my sample GL HTML page</a> and copy the <a href="/assets/js/webgl-utils.js">webgl-utils.js</a> file into a subdirectory called &lsquo;js&rsquo; under that. Run the HTML file in your browser and you&rsquo;ll have a WebGL app!</p>

<h2>Understanding the OpenGL API</h2>

<p><em>Note:</em> I&rsquo;m using OpenGL ES 2 GLSL syntax in my examples because I believe that&rsquo;s probably got the broadest platform support, and is most similar to WebGL. The concepts are the same for later versions, aside from the syntactic differences. As I am focusing on explaining core concepts only OpenGL ES 2 should be fine for my purposes.</p>

<p>Take a moment to read some OpenGL specifications &ndash; they are probably easier to understand than you&rsquo;d think. Here&rsquo;s the <a href="https://www.khronos.org/registry/gles/specs/2.0/es_full_spec_2.0.25.pdf">OpenGL ES 2.0 Spec</a> if you want a definitive source for all this stuff.</p>

<p>I will probably mention the Khronos Group a lot throughout this article. The <a href="http://khronos.org">Khronos Group</a> is a consortium of companies such as Sun Microsystems, NVidia and Silicon Graphics who work on standardising graphics APIs, including OpenGL. Part of their OpenGL standardisation process is to provide reference interfaces (header files) for each version of the API that vendor implementations should conform to.</p>

<p>Here&rsquo;s a bit of a glossary of terms and concepts that are necessary to become familiar with in order to be an effective OpenGL programmer.</p>

<h3>The OpenGL API &ndash; commands, enums and objects</h3>

<p>The OpenGL API consists entirely of commands (e.g., <code>glDrawElements()</code>), enums (e.g., <code>GL_COLOR_BUFFER_BIT</code>) and, conceptually, objects.</p>

<p>The <strong>commands</strong> and <strong>enums</strong> are described in the specification but can also be browsed on the Khronos API header files &ndash; have a look at <a href="https://www.khronos.org/registry/gles/api/GLES2/gl2.h">GLES2/gl2.h</a> for the standard GLES2 header. Most vendor implementations use these exact files as the entry point for their libraries.</p>

<p>OpenGL <strong>objects</strong> are the conceptual entities you create using the API, such as vertex buffers (VBOs), vertex arrays (VAOs), shaders, programs, textures and framebuffers. OpenGL has commands to create, bind and delete each of the different types of objects. Note that the objects you create are only valid with the context that was active when you created them!</p>

<p>OpenGL commands that operate on OpenGL objects require those objects to be <em>bound</em>. For example when you call <code>glDrawElements()</code> you don&rsquo;t pass the program, the target framebuffer or the vertex or index buffer IDs to the function, it just operates on whichever program, framebuffer and vertex buffers are currently bound (through the <code>glUseProgram()</code>, <code>glBindFramebuffer()</code> and <code>glBindBuffer()</code> commands.) Many people feel this is a confusing and error-prone way to do things; it means that if you call some library or function that uses OpenGL there is no way to determine whether it has modified the current context in any way, so you have to re-bind all your stuff. It&rsquo;s unfortunate but that&rsquo;s the API we are stuck with :(</p>

<p><em>Note:</em> it is interesting to note that the <code>glCreate</code>-type functions don&rsquo;t actually create or allocate anything! They give you back a free handle, but that handle isn&rsquo;t actually allocated until it is bound. This means that technically you&rsquo;re free to keep track of these IDs yourself and allocate them according to whatever scheme you most desire, though I&rsquo;m not sure why you&rsquo;d do that.</p>

<h3>OpenGL context</h3>

<p><em>Here I&rsquo;ll describe the concepts involved in creating and using OpenGL contexts. For a more tutorial-type approach have a look at <a href="https://open.gl/context">this guide</a> that shows how to use several different popular context management libraries.</em></p>

<p>The <strong>context</strong> encapsulates the rendering view, its rendering settings and which OpenGL <em>objects</em> are currently active (such as which shader will be invoked during rendering). When starting your application you will need to create a context and set the capabilities you want the driver will use while rendering, such as whether the renderer will perform scissor, stencil, depth or alpha testing, how the renderer will blend fragments into final pixel colours and what part of the window to render into (the viewport). Contexts are not thread-safe (unfortunately!) and are essentially global in scope (even more unfortunately!) and are the cause of most of the grief people have with OpenGL.</p>

<p>You should avoid querying the state of the context too much if possible (i.e., querying information about bound objects, or even constantly querying error state) &ndash; it is apparently quite slow. You should also avoid accessing context from multiple different threads. I believe you are supposed to create separate contexts in each thread you wish to render from, or more preferably not render from multiple threads at all.</p>

<p>It is also possible to create a <em>shared context</em> where objects created on one context are available in another. This can be tricky so proceed with caution &ndash; for example, according to the spec shared contexts may not share framebuffer objects for some reason. I found that out <a href="https://code.google.com/p/angleproject/issues/detail?id=979">the hard way</a>.</p>

<p>Unfortunately the creation of an OpenGL context is not defined by the OpenGL spec. If you want to create one you&rsquo;ll either have to look up how to do this on your platform of choice (for example Windows provides WGL, while <a href="http://www.geeks3d.com/20121109/overview-of-opengl-support-on-os-x/">OSX</a> and Linux systems use GLX) or use another third-party library that does this for you (I like GLFW, but SDL2 is very popular, and some people still use the older GLUT.) These libraries often give you access to keyboard and mouse input as well as various other utilities you might need when making your app (such as buffer swapping, text, audio and the like.)</p>

<p>Below is a simple example of creating a context using GLFW in C or C++. For a line-by-line explanation of this see <a href="http://www.glfw.org/docs/latest/quick.html">the official GLFW docs</a>.</p>

<p>``` c++ Creating a context with GLFW</p>

<h1>include &lt;GLFW/glfw3.h></h1>

<p>static void key_callback(GLFWwindow* window, int key, int scancode, int action, int mods) {</p>

<pre><code>if (key == GLFW_KEY_ESCAPE &amp;&amp; action == GLFW_PRESS)
    glfwSetWindowShouldClose(window, GL_TRUE);
</code></pre>

<p>}</p>

<p>int main() {
  if (!glfwInit())</p>

<pre><code>  exit(EXIT_FAILURE);
</code></pre>

<p>  // create context (unfortunately GLFW bundles this in with window creation)
  GLFWwindow* window = glfwCreateWindow(640, 480, &ldquo;Simple example&rdquo;, NULL, NULL);
  if (!window) {</p>

<pre><code>glfwTerminate();
exit(EXIT_FAILURE);
</code></pre>

<p>  }
  glfwMakeContextCurrent(window);
  glfwSwapInterval(1); // wait for a vsync before swapping to avoid &lsquo;tearing&rsquo;</p>

<p>  // tell GLFW to notify us when keys are pressed (esc will exit)
  glfwSetKeyCallback(window, key_callback);</p>

<p>  /////
  // OpenGL configure context capabilities
  glClearColor(1., 0., 0., 1.);
  glEnable(GL_DEPTH_TEST);
  glEnable(GL_BLEND);
  glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);</p>

<p>  // TODO: create OpenGL objects (shader programs, vertex buffers, etc) here
  ///</p>

<p>  // main loop
  while (!glfwWindowShouldClose(window)) {</p>

<pre><code>int width, height;
glfwGetFramebufferSize(window, &amp;width, &amp;height);

/////
// OpenGL render
glViewport(0, 0, width, height);
glClear(GL_COLOR_BUFFER_BIT);

// TODO: draw your primitives here!
///

glfwSwapBuffers(window);
glfwPollEvents();
</code></pre>

<p>  }</p>

<p>  // TODO: destroy OpenGL objects here</p>

<p>  glfwDestroyWindow(window);
  glfwTerminate();</p>

<p>  exit(EXIT_SUCCESS);
}
```
Most OpenGL context management libraries are very similar in usage to this.</p>

<h3>Primitives, Vertices and Fragments</h3>

<p>Each time you call an OpenGL drawing function you are drawing a <strong>primitive</strong>. You can think of a primitive as a shape of some kind that can be either 2D or 3D and rendered as a collection of points, lines or triangles.</p>

<p>You specify the primitive type you want to draw when invoking <code>glDrawElements()</code> or <code>glDrawArrays()</code>. Among the valid types are:</p>

<ul>
<li><code>GL_TRIANGLES</code>, which uses each 3 vectors to create a triangle. If you provide 9 vertices you will draw 3 triangles.</li>
<li><code>GL_TRIANGLE_STRIP</code> is more complicated in that it will use each set of 3 consecutive vertices to draw a triangle. In other words if you supply 4 vertices it will draw 2 triangles &ndash; one from v0, v1 and v2, another from v2, v1, v3. Notice that the first two vertices of every second triangle are re-ordered &ndash; this is to ensure that the triangles all face the same direction, as triangle direction is derived from which direction the triangle&rsquo;s vertices appear to be counter-clockwise in order (relevant if face culling is enabled.)</li>
<li><code>GL_POINTS</code>, which renders unconnected dots. You can control the size of the points by calling the <code>glPointSize()</code> command in desktop GL or by setting the <code>gl_PointSize</code> GLSL variable in OpenGL ES. You can also apply a texture to the point to render more complex particles (the GPU passes your fragment a <code>gl_PointCoord</code> variable to indicate which texture UV coordinates you should render.)</li>
<li><code>GL_LINES</code> is useful for quickly drawing a wireframe of your model.</li>
<li>there are also <code>GL_LINE_STRIP</code>, <code>GL_LINE_LOOP</code>, <code>GL_TRIANGLE_FAN</code>, <code>GL_QUADS</code>, <code>GL_QUAD_STRIP</code> and <code>GL_POLYGON</code>.</li>
</ul>


<p>You will describe your primitives as collections of <strong>vertices</strong>. These vertices are passed to the GPU and it then transforms them to triangles then <em>rasterises</em> those triangles into <strong>fragments</strong>. These fragments may be filtered, blended and anti-aliased and ultimately may be drawn as pixels on your screen. So technically <em>fragments are not pixels</em>.</p>

<h3>Coordinate systems</h3>

<p>The OpenGL <strong>coordinate system</strong> is simple but requires some explanation &ndash; put simply, the range of visible coordinates within the viewport goes from -1.0 to 1.0 in the X, Y and Z directions. This coordinate space is known as <strong>normalized device coordinates</strong> or NDC, and anything falling outside of this range will not be rendered. The X and Y coordinates describe the horizontal and vertical component of the pixel (-1, -1 corresponds with the bottom left corner of the viewport) and the Z axis is the <em>depth</em> component that is used for depth testing (if enabled.) By default the NDC Z coordinates move <em>away from</em> the viewer, so +Z is into the screen.</p>

<p><img class="center" src="/images/upload/2015-05-09-gl_1_ndc.png" title="&ldquo;Normalised device coordinates (NDC). By convention XYZ shown as RGB&rdquo;" ></p>

<p>But <strong>you won&rsquo;t be using NDC directly</strong> &ndash; you will be rendering your primitives in what is known as <em>clip coordinates</em>, which are <em>very</em> similar to NDC except with a 4th dimension (x, y, z and w). You might be somewhat confused when you write your vertex shader, when setting the mandatory <code>gl_Position</code> variable with the vertex coordinates (described later) you&rsquo;ll notice it is a <code>vec4</code>. What&rsquo;s the 4th dimension for? It turns out the 4th dimension <em>w</em> is used for <em>perspective division</em>, which is super useful for 3D graphics but useless for 2D. For now you just have to know that when the GPU transforms from <em>clip coordinates</em> to <em>NDC</em> it calculates something like this: <code>ndc_coords = clip_coords.xyz / clip_coords.w</code>. So if rendering 2D stuff to the screen just set <code>gl_Position.w</code> to 1.0.</p>

<p>When programming in 2D you may choose to draw all your primitives using these NDC coordinates to avoid having to convert between coordinate spaces at all. The samples in this article draw primitives using normalised device coordinates directly.</p>

<p>3D coordinate systems are more complicated and I will discuss them in depth in a later article. For now it is interesting to note two things: first, by convention OpenGL coordinate systems other than NDC generally have the Z axis moving <em>towards</em> the viewer, so the <em>negative</em> Z axis goes into the screen.</p>

<p>Secondly, when rendering 3D primitives the verticies that describe the primitive are usually transformed in between a well defined sequence of coordinate spaces. The coordinates in the model&rsquo;s local <strong>model space</strong> are first moved to <strong>world space</strong> where their position and orientation is given relative to all other objects in a scene (the same model may be used many times in the one scene, but each will look different if they have different model space transformations.) Then the coordinates are transformed to <strong>view space</strong> where their position and orientation are relative to the viewer&rsquo;s eye. Then they are transformed into clip coordinates and finally NDC as before. I hope to go into more detail on this process in a later article &ndash; I just wanted to list the terms here for completeness.</p>

<p>One final thing I will mention about 3D coordinates is that it is common to specify 3D positions, directions and transformations using 4 dimensional vectors and matricies. I won&rsquo;t go into it now but it is very useful to remember that <em>positional</em> coordinates generally have a 4th dimension <em>w</em> set to 1.0 and <em>directions</em> generally have <em>w</em> set to 0. This makes the linear algebra work out nicely when multiplying against transformation matricies as the 4th dimension in the vector usually controls the translation factor of the transformation, which is not relevant for directions (i.e., a &lsquo;north&rsquo; pointing vector is stil pointing north no matter where the vector is located in space.)</p>

<h3>Shaders and the render pipeline</h3>

<p>Every time you call an OpenGL draw operation (the <code>glDrawArrays()</code> or <code>glDrawElements()</code> commands) you invoke the entire render pipeline. This is when the GPU passes the verticies in your primitives to your vertex shader, clips the resulting coordinates, generates triangles from the vertices, rasterises them into fragments, passes those fragments to your fragment shader then tests visibility, blends and dithers those fragments into pixels on your screen (or some other framebuffer.)</p>

<p>The difference between <code>glDrawArrays()</code> and <code>glDrawElements()</code> is in how the GPU knows which vertices to use when transforming your vertices into triangles. <code>glDrawArrays()</code> is simpler in that it builds triangles using the vertices in their given order &ndash; in a GL_TRIANGLES primitive, triangle T0 will be built from vertices V0, V1 and V2, triangle T1 built from V3, V4, V5 and so on.</p>

<p><code>glDrawElements()</code> is more complex but often more performant. Instead of using the vertex data strictly in the order you declared them, it uses a separate buffer called the <em>index buffer</em> (also known as the <em>element buffer</em>) to determine which vertices to use in which triangles. This is great because it allows you to reuse vertices to create multiple triangles &ndash; an index buffer of <code>[0, 1, 2, 2, 1, 3, 1, 0, 4]</code> will construct 3 triangles from only 5 vertices! The first triangle will be using v0, v1 and v2, the second using v2, v1, v3 and the third using v1, v0 and v4.</p>

<p><em>Note:</em> Vertex buffers are bound by invoking <code>glBindBuffer(GL_ARRAY_BUFFER, my_buff_loc)</code> and index buffers are bound with <code>glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, my_buff_loc)</code>. Don&rsquo;t ask me about the names.</p>

<h4>Pipeline summary</h4>

<p>To draw a primitive, the GPU first needs your <em>vertex data</em>. The GPU will decode your vertex data to extract <em>vertex attributes</em> and pass those into your <em>vertex shader</em> once for each vertex. Your vertex shader is expected to output clip coordinates for that vertex so the GPU knows where on the screen to show it (if at all,) and information that the GPU should pass on to your fragment shader for fragments derived from this vertex.</p>

<p>Once the GPU has processed all vertices the GPU can clip the vertices to within the NDC area only, it transforms those vertices into triangles and then <em>rasterises</em> the triangles into fragments which sort-of represent the pixels of the primitive being drawn. The GPU will then call your <em>fragment shader</em> for each fragment, passing it the relevant output data of your <em>vertex shader</em>. Your fragment shader is expected to output a colour and optionally a new depth-value for the fragment.</p>

<p>The GPU then processes, blends and dithers the fragments to the output framebuffer, as described under <em>how the GPU writes fragments to the framebuffer</em> a few pages down.</p>

<p>You invoke the rendering pipeline by calling either the <code>glDrawArrays()</code> or <code>glDrawElements()</code> commands (or one of their variations.) Unless you&rsquo;re using the fixed function pipeline you&rsquo;ll have to have a shader program bound so the GPU knows which vertex shader and fragment shader to invoke. You will also have to specify the vertex data (the vertex positions and perhaps other information such as the vertex normals and colours) of the primitive you are rendering. The &lsquo;basic shader program&rsquo; a few pages down shows one way to do both of these things.</p>

<p><center><img src='/images/plantuml/c5680b261dbef74198e842be690657ca.png'></center></p>

<p>Once again, if you want more details on how this works have a look at the specs, like the <a href="https://www.khronos.org/registry/gles/specs/2.0/es_full_spec_2.0.25.pdf">OpenGL ES 2.0 Spec</a>.</p>

<h4>Passing data to the GPU &ndash; uniforms, attributes and varyings</h4>

<p>There are two ways your application can pass data to your shaders &ndash; <em>uniforms</em> that are set only once per <code>glDraw*</code> call and and <em>attributes</em> that may be different per-vertex. In addition, fragment shaders receive per-fragment input derived from the output of the vertex shader in <em>varyings</em> (see the <em>rendering pipeline</em> diagram above.)</p>

<h5>Sending uniform data to the GPU</h5>

<p>A <em>uniform</em> represents a variable that remains the same for the rendering of an entire primitive. This might be something like the object material or the position of the sun. The pattern for setting a uniform is to first get a handle to the uniform with <code>glGetUniformLocation(my_program, "my_uniform")</code>. Pass this handle to the <code>glUniform*</code> functions to set the uniform (for example <code>glUniform3f()</code> will set a uniform of type <code>vec3</code> in your shader.) Setting a uniform will make it available in any of the shaders in the bound program that are want to use it.</p>

<p>In the code example a few pages down you can see how the unifrom data is bound and updated:
<code>c++ uniform updating snippet
  auto time_loc = glGetUniformLocation(shaderProgram, "time");
  glUniform1f(time_loc, glfwGetTime());
</code></p>

<p>Uniforms may also be structures and arrays &ndash; the syntax to declare and use structs and arrays in GLSL (the shader code) is very similar to C, but setting them from the client application is a little tricky.</p>

<p><em>To set uniform values in a structure</em>: you essentially treat the variable as if it is in the namespace of the structure name. E.g., if the shader contains the code <code>struct Point { float x; float y; }; uniform Point p1;</code> you can access <code>p1.x</code> with exactly that syntax: <code>auto u_p1x = glGetUniformLocation(my_program, "p1.x")</code>.</p>

<p><em>To set uniform values in an array</em>: you access the specific array element using standard C syntax. If the shader contains the code <code>float xs[10];</code> you can get the location of a particluar element of <code>xs</code> with <code>auto u_xs = glGetUniformLocation(my_program, "xs[0]")</code>. You can use this uniform location to either set a single element using <code>glUniform*()</code> or set a number of the elements using <code>glUniform*v()</code>.</p>

<p><em>Note:</em> that you cannot apply offsets to the returned uniform location to access specific array elements &ndash; in the above example, I cannot call <code>glUniform1f(u_xs + 2, 1.)</code> as that could be the location of a totally different uniform. In this case you would either have to find the location of the element you want to access directly (in this case <code>glGetUniformLocation(my_program, "xs[2]")</code>) or set multiple elements (using <code>glUniform*v()</code>) in the array starting from the index we retrieved.</p>

<h5>Sending vertex attribute data to the GPU</h5>

<p>An <em>attribute</em> represents data related to the current vertex being processed. You specify where the GPU can find vertex data by calling the <code>glVertexAttribPointer*</code> functions for each vertex attribute in your vertex shader. This process is <em>very</em> different to specifying uniforms!</p>

<p>The standard form for using a vertex attribute is something like:
``` c++ setting vertex attribute data
// &hellip; first bind the appropriate shader</p>

<p>// during initialisation
GLint position_loc = glGetAttribLocation(shaderProgram, &ldquo;position&rdquo;);
glEnableVertexAttribArray(position_loc);
const float vertex_buffer[] = { 0., 0., 0., &hellip; }; // x, y, z positions
glVertexAttribPointer(position_loc, 3, GL_FLOAT, false, 0, vertex_buffer);
glDisableVertexAttribArray(position_loc);</p>

<p>// at render time
glEnableVertexAttribArray(position_loc);
glDrawArrays(GL_TRIANGLES, 0, 4); // draw verts 0..4 as triangles
glDisableVertexAttribArray(position_loc);
<code>``
The above code snippet illustrates how to passing a raw pointer to your vertex data to the GPU. This is not the usual case, because quite often your buffered data will interleave more than one attribute's worth of information (such as normals, vertex colours etc.) Usually you will create a __vertex buffer object__ (VBO) and use</code>glVertexAttribPointer()` to dictate how the GPU should extract the vertex info from that.</p>

<p><em>vertex buffer objects</em> are created, bound and destroyed like any other OpenGL object. You specify the data to buffer by calling the <code>glBindBuffer()</code> command with a pointer to the buffer. While this buffer is bound, any calls to <code>glVertexAttribPointer()</code> with a non-pointer in the final parameter will implicitly be referring to the bound buffer, using the final parameter instead as an offset into the buffer where the data may be found. This is necessary for interleaving vertex data in the same buffer.</p>

<p>Having a single buffer with different types of vertex information interleaved is very common. Your two tools for describing how this buffer data is formatted are the above-mentioned <em>offset</em> parameter and the <em>stride</em> parameter. While the <em>offset</em> describes the starting byte of an attribute in the buffer, the <em>stride</em> describes the total distance (in bytes!) between the start of that attribute and the start of the next instance of that attribute. A stride of <code>0</code> is considered special &ndash; it is used if the attribute is &lsquo;tightly packed&rsquo;, meaning the buffer contains only that attribute with no space between them.</p>

<p>This is best illustrated with an example:
``` c++ specifying attributes in an interleaved buffer</p>

<pre><code>float positions_and_colours_buffer[] = {
  -1., 0.,   1., 0., 0.,   // x, y,   r, g, b
  -1., -1.,  0., 1., 0.,   // x, y,   r, g, b
  // ...
};
glBindBuffer(GL_ARRAY_BUFFER, positions_and_colours_buffer);

auto position_offset = 0U;  // positions start 0 bytes in
auto colour_offset = 2 * sizeof(GLfloat); // normals start 8 bytes in
int stride = 5 * sizeof(GLfloat);  // each vertex is total 10 bytes

glVertexAttribPointer(position_loc, 3, GL_FLOAT, false, stride, (void*)position_offset);  
glVertexAtrirbPointer(colour_loc, 3, GL_FLOAT, false, stride, (void*)colour_offset);
</code></pre>

<p>```</p>

<p><em>Note</em>: in OpenGL 3.0 and above you will want to use <strong>vertex array objects</strong> (VAOs) to speed up your rendering process. A <em>VAO</em> offers you shorthand for binding vertex bufffers and the related vertex attributes for those buffers. This means that <code>glBindVertexArray()</code> can replace a number of <code>glBindBuffer()</code> and <code>glEnableVertexAttribArray()</code> commands.</p>

<p>The pattern for using this is:
``` c++
// at init time when creating buffers
glBindVertexArray(my_vao);
  glBindBuffer(my_vbo);
  GLfloat verts[] = {0., 0.,  .5, .5,  0., .5 };
  glBufferData(GL_ARRAY_BUFFER, sizeof(verts), verts, GL_STATIC_DRAW);
  glEnableVertexAttribArray(position_loc);
  glVertexAttribPointer(position_loc, &hellip;);
glBindVertexArray(0);</p>

<p>// at render time
glBindVertexArray(my_vao);
  glDrawElements(&hellip;);
glBindVertexArray(0);</p>

<p>```
I won&rsquo;t be illustrating those here because I am focusing on OpenGL ES 2.0 for this article.</p>

<h5>Sending per-fragment varyings to the fragment shader</h5>

<p><em>Varyings</em> are per-fragment data the fragment shader uses to calculate the output colour of a fragment. Examples of this might be the normal of the surface at that point, or the material colour interpolated from the material colours of the surrounding vertices. A 3D program will often use this information along with the location of the sun (passed through a uniform) in its lighting calculations &ndash; a fragment on a surface directly facing a light source will have a brighter colour than one not.</p>

<p>The calculation of what value is passed into a varying is a little bit tricky. Fragment shaders get their per-fragment input indirectly from the vertex shaders through variables called <em>varyings</em>. Of course for any three vertices forming a triangle you could have hundreds of fragments, so the GPU takes the varyings coming out of the vertex shader for each vertex influencing a fragment and interpolates them before passing them into the fragment shader.</p>

<h4>The Vertex Shader</h4>

<p>Below is a simple vertex shader that expects two input variables from the application: the uniform <code>time</code> and the vertex attribute <code>vert_position</code>.</p>

<p>``` c++ a simple vertex shader
// per-primitive variables passed in from your application
uniform float time;</p>

<p>// per-vertex variables passed in from your application
attribute vec4 vert_position;</p>

<p>// output data to send to the fragment shader for each fragment derived from this vertex
varying vec4 frag_colour;</p>

<p>void main() {
  // mandatory! calculate the NDC coordinates of this vertex
  gl_Position = vec4(0.01 * sin(time), 0., 0., 0.) + vert_position;
  // frags go from black on the left side to red on the right side of the viewport
  frag_colour = vec4((vert_position.x + 1.) / 2., 0., 0., 1.);
}
<code>``
_Note:_ GLSL (the shader language) allows the specification of _precision_ for floating-point values, including all</code>vec<code>and</code>mat<code>types. Some versions of GLSL (at least GLSL ES) _require_ variable precisions to be specified in all declarations and parameter lists (I have omitted these for brevity in my examples.) Valid precision values are</code>lowp<code>,</code>mediump<code>and</code>highp<code>. In general</code>mediump<code>is what you want, although for colours you can use</code>lowp` without any problems.</p>

<h4>The Fragment Shader</h4>

<p>The fragment shader captures all the logic required to determine the colour of a fragment. This might be as simple as just returning a uniform RGBA value or might involve complex 3D shading calculations incorporating a number of light sources and shadow maps. I will explore the 3D stuff in a later article.</p>

<p>A simple fragment shader that works with the above vertex shader might look like this:
``` c++ a simple fragment shader
// per-primitive variables passed in from your application
uniform float time;</p>

<p>// interpolated from output data of vertex shader
varying vec4 frag_colour;</p>

<p>void main() {
  gl_FragColor = frag_colour;
}
```</p>

<p>Note that the <code>varying</code> variables are not passed directly from the vertex shader but is actually interpolated from the results of all vertex shader invocations for the vertices surrounding this fragment. This means that, for example, colour gradients look smooth between vertices.</p>

<h4>How the GPU writes fragments to the FrameBuffer</h4>

<p>In this context the <em>framebuffer</em> is the rendering target, which is usually either the viewport or a texture. (Framebuffers are also used for other purposes such as combining multiple rendering passes that I will go into in a later article.) When the GPU has collected all the fragments it is going to render, it goes through a series of per-fragment operations to determine what gets written to the framebuffer.</p>

<p>First the GPU checks to ensure this bit of the viewport is actually owned by this framebuffer, because is possible to have one viewport obscuring another. Next the fragment is tested against the scissor test region, the stencil buffer, then the depth buffer, if those capabilities are enabled in current context. If a fragment fails one of these tests it is discarded. Then the GPU performs a blending operation if enabled on the context, blending the fragment against what is read from the framebuffer before the render operation began. The resultant fragment is finally written to the framebuffer. Furthermore, if the framebuffer has <em>multisampling</em> enabled (for anti-aliasing) it may merge multiple fragment colours (or <em>samples</em> as they are called now) into a single pixel.</p>

<h4>Basic shader program example</h4>

<p>The partial application code below shows a basic vertex shader, fragment shader being invoked to render a square in the middle of the window.</p>

<p>``` c++ rendering a &lsquo;triangle strip&rsquo; primitive using a buffer of vertex positions
  // just define our shader in-line
  const char vertex_src[] = &ldquo;\</p>

<pre><code>uniform float time;       // passed in for whole primitive \n
attribute vec2 position;  // passed in with vertex data \n
attribute vec3 colour;    // passed in with vertex data \n
varying vec4 frag_colour; // passed out to frag shader  \n
void main() {                                           \n
  // this is where we would transform to NDC, but  our coordinates are already NDC
  // so just pass the position through as-is with a little animation \n
  gl_Position = vec4(position * sin(time), 0., 1.);     \n
  frag_colour = vec4(colour, 1.);
</code></pre>

<p>  }&ldquo;;
  const char fragment_src[] = &rdquo;\</p>

<pre><code>varying vec4 frag_colour; // passed in from vert shader (and interpolated) \n
void main() {                              \n
  // fragment colour is dark gray          \n
  gl_FragColor = frag_colour;
</code></pre>

<p>  }&ldquo;;</p>

<p>  // create shaders
  auto vertexShader = glCreateShader(GL_VERTEX_SHADER);
  glShaderSource(vertexShader, 1, &amp;vertex_src, NULL);
  glCompileShader(vertexShader);
  auto fragmentShader = glCreateShader(GL_FRAGMENT_SHADER);
  glShaderSource(fragmentShader, 1, &amp;fragment_src, NULL);
  glCompileShader(fragmentShader);</p>

<p>  // create shader program using the shaders
  GLuint shaderProgram = glCreateProgram();
  glAttachShader(shaderProgram, vertexShader);
  glAttachShader(shaderProgram, fragmentShader);
  glLinkProgram(shaderProgram);    // link the program
  glUseProgram(shaderProgram);    // and select it for usage</p>

<p>  // these are the NDC coordinates of a square on the viewport
  static const float vertexArray[] = {</p>

<pre><code> 0.0, 0.5,   1., .5, .5,    // x, y,  r, g, b,
 -0.5, 0.0,  .5, 1., .5,    // x, y,  r, g, b,
 0.0, 0.5,   1., .5, .5,    // x, y,  r, g, b,
 0.0, -0.5,  .9, .9, .9,    // x, y,  r, g, b,
 0.5, 0.0,   .5, .5, 1.,    // x, y,  r, g, b,
</code></pre>

<p>  };</p>

<p>  // we need this to pass the &lsquo;time&rsquo; uniform to the shaders
  auto time_loc = glGetUniformLocation(shaderProgram, &ldquo;time&rdquo;);
  // we need this to pass the &lsquo;position&rsquo; and &lsquo;colour&rsquo; attributes in to the vertex shader
  auto position_loc = glGetAttribLocation(shaderProgram, &ldquo;position&rdquo;);
  auto colour_loc = glGetAttribLocation(shaderProgram, &ldquo;colour&rdquo;);
  glEnableVertexAttribArray(position_loc);
  glEnableVertexAttribArray(colour_loc);
  // glVertexAttribPointer allows you to specify vertices in many ways, so its pretty complicated
  glVertexAttribPointer(position_loc, 2, GL_FLOAT, false, sizeof(GLfloat) * 5, sizeof(GLfloat) * vertexArray);
  glVertexAttribPointer(position_loc, 3, GL_FLOAT, false, sizeof(GLfloat) * 5, sizeof(GLfloat) * (vertexArray + 2));
  glDisableVertexAttribArray(position_loc);
  glDisableVertexAttribArray(colour_loc);</p>

<p>  while (!quit) {</p>

<pre><code>// ... clear viewport etc 

glUniform1f(time_loc, glfwGetTime());

glEnableVertexAttribArray(position_loc);
glEnableVertexAttribArray(colour_loc);
glDrawArrays(GL_TRIANGLE_STRIP, 0, 5);
glDisableVertexAttribArray(position_loc);
glDisableVertexAttribArray(colour_loc);

// ... swap buffers etc
</code></pre>

<p>  }
```
This should look something like this:</p>

<center>
  <iframe src="http://seshbot.com/assets/2015-05-13-gl1.html" width="320" height="200" scrolling="no" style="border: 2px solid black; -moz-box-shadow: black 2px 2px 2px;"></iframe>
  <br/>
  <a style="clear:both;" href="http://seshbot.com/assets/2015-05-13-gl1.html" target="_blank">click here to open in a separate window</a>
</center>


<h3>Immediate mode and the fixed function pipeline</h3>

<p>I discuss this more when discussing the different OpenGL versions below, but OpenGL 1 was much simpler to use than later versions, though much less powerful. OpenGL 1 operated using a <em>fixed-function pipeline</em> using an <em>immediate mode</em> API, where the programmer not only describes high-level primitives' individual vertices but also describe the lighting model to use, define several lights and set up materials to use during rendering. Retained mode allows all of this functionality to be executed on a shader program, which is written by the developer but run on the GPU directly. This is much more performant but requires a lot of extra work on the part of the developer.</p>

<p>The term immediate mode means that the drawing operations are explicitly executed in your client application each frame, which is considered slower because it ties the client application logic too closely with the GPU rendering operations, so the GPU is not able to make as many optimisations as it would if the instructions were on the GPU itself (retained mode.)</p>

<h2>Linear algebra (magic!)</h2>

<p>Linear algebra is the language of graphics programming. You <em>need</em> to learn some basics if you&rsquo;re going to tackle this stuff. I won&rsquo;t go into what a vector or matrix is here but you will have to learn the basics of their form and function if you don&rsquo;t already know them.</p>

<p>The most basic understanding you should have is that vectors are usually used to describe coordinates in space and directions, and matricies are used to describe transformations (translation, scale, rotation, shear etc) to those vectors. Another thing to note is that a single matrix may represent an accumulation of many different transformations performed in sequence, so if I said (in pseudo-code) <code>auto m = translate * scale * rotate</code>, then any time I multiply <code>m</code> by a vector it will have the same effect as performing all of those transformations at once &ndash; amazing!</p>

<p>Once again, the OpenGL API does not help you in dealing with matricies or vectors, but there is a great supporting library that does &ndash; <a href="http://glm.g-truc.net/">GLM</a>.</p>

<p>There are two ways the elements in a matrix may be stored &ndash; OpenGL programmers often use <em>column-major</em> matrix layouts. This is a convention only, but is generally used in the official documentation and in OpenGL support libraries such as GLM. The reason this is important is that unlike scalar multiplication, matrix multiplication is not <em>commutative</em>, meaning <code>A * B</code> does not equal <code>B * A</code>. The main impact of using column-major vs row-major matricies is the order of multiplications must be reversed to have the same effect. In column-major (the most usual) you would accumulate your transformations to the left, so if you want to first rotate (<em>R</em>) then scale (<em>S</em>) then translate (<em>T</em>) last, you would execute <code>T * S * R</code>. A more common example would be when calculating the <em>model view projection</em> matrix it would be accumulated as <code>mat4 mvp = P * V * M</code>. When calculating this with a <em>row-major</em> library you would be expected to accumulate it as <code>mat4 mvp = M * V * P</code>. Converting a matrix to or from column-major to row-major is known as <em>transposing</em> the matrix.</p>

<p>A common term in linear algebra is the <em>identity matrix</em>. This is a matrix <em>I</em> where multiplying it with another matrix <em>A</em> gives that matrix (<code>I * A = A</code>.) It is easily recognisable as it is entirely made of 0s with 1s on the diagonal.</p>

<p>Another generally useful matrix operation is the <em>inverse</em> of a matrix. The inverse of a matrix A is the matrix required to multiply with A so that the result is the identity matrix. In other words, <code>Ai * A = I</code>. This is useful when rolling back a matrix multiplication. If you have the <em>model view projection</em> matrix <code>mat4 mvp = proj * view * model</code>, you can find the <em>model view</em> matrix by calculating the inverse projection <em>inv_proj</em> and calculating: <code>mat4 mv = inv_proj * mvp</code>.</p>

<p>Vector operations are even more interesting. A few things I want to point out here are <em>dot product</em>, <em>cross product</em> and the difference between <em>positional coordinates</em> and <em>directional coordinates</em>.</p>

<p>The <strong>dot product</strong> operation (sometimes known as the <em>inner product</em>) is used for many purposes; the dot product of two vectors A and B is a scalar (not a vector) number that is the sum of the products of their components (e.g., <code>auto a_dot_b = A.x * B.x + A.y * B.y + A.z * B.z</code>). It turns out that this simple formula gives you the cosine of the angle between those vectors multiplied by their magnitudes (<code>|A||B|cos(Ɵ)</code>), which is really useful because:</p>

<ul>
<li>if the vectors are unit vectors (they each have magnitude of 1) the dot product will just give you <code>cos(Ɵ)</code> which is a number between 0 and 1, where 0 implies that they are perpendicular to each other (at 90 degrees) and 1 implies they are parallel. This is great for calculating how much light should bounce off a surface if the light direction is one vector and the surface normal is the other.</li>
<li>if the vectors are both unit vectors you can inverse cos the dot product to find the angle between the vectors (<code>auto angle = acos(dot(A, B))</code>)</li>
<li>you can find the projection of vector A onto vector B by finding the dot product of A and B then dividing the result by the length of A.</li>
<li>calculating the dot product of a vector with itself will give you the distance squared. If you are checking to see which vector is longer, you can just compare their squared distances (saving you a square root operation)</li>
</ul>


<p>The <strong>cross product</strong> is another simple formula that gives you a vector that is perpendicular to two given vectors. In other words, if you have vectors A and B that both lie along the same surface, calculating <code>cross(A, B)</code> will give a vector that represents the normal to that surface. This normal vector will also follow the <em>right-hand rule</em> as pictured below (Note: you will usually want to normalise your normal before using it, so lighting calculations can dot product them straight out of the box!)</p>

<p><img class="center <a" src="href="http://upload.wikimedia.org/wikipedia/commons/d/d2/Right_hand_rule_cross_product.svg">http://upload.wikimedia.org/wikipedia/commons/d/d2/Right_hand_rule_cross_product.svg</a>" width="200" height="200" title="&ldquo;Cross product follows the right-hand rule&rdquo;" ></p>

<h2>OpenGL versions and extensions</h2>

<p>A constant frustration when reading documentation and code examples is that OpenGL 1.0 is <em>worlds apart</em> from OpenGL 2.0. Many of us would have been well served if they had given it a totally different name, so different are the products!</p>

<p><strong>OpenGL 1</strong> functions in what is now called <em>immediate mode</em> and is considered deprecated. In this version the programmer describes the scene lights, materials and fog, then notifies the driver of each polygon explicitly. The driver would then render the scene using its own internal lighting model using the described lights and materials. This is called the <em>fixed function pipeline</em>.</p>

<p>``` c++ OpenGL 1 example code snippet
// configure light
glEnable(GL_LIGHTING);     // deprecated
glEnable(GL_LIGHT0);       // deprecated
glLightfv(GL_LIGHT0, GL_AMBIENT, {.4f, .1f, .1f}); // deprecated</p>

<p>// draw a primitive
glPushMatrix();            // deprecated
// matrix operations will apply to all vertices until glPopMatrix()
glLoadIdentity();          // deprecated
glRotatef(3.14f, 0., 0., 1.); // deprecated
glBegin(GL_TRIANGLE);      // deprecated
  glVertex3f(.0, .0., .0); // deprecated
  glVertex3f(.1, .0., .0); // deprecated
  glVertex3f(.1, .1., .0); // deprecated
glEnd();                   // deprecated
glPopMatrix();             // deprecated
glFlush();
```</p>

<p><strong>OpenGL 2</strong> introduced shaders, yet still provided compatability with the above described model. Shaders had access to the state declared in the fixed function pipeline through standard global variables (such as the <code>gl_LightSource[]</code> array and the <code>gl_FrontMaterial</code> variable sent from the client and the <code>gl_ModelViewProjection</code> matrix which was generated by the driver.) The vertex shader can invoke the standard fixed-function pipeline functionality by callling <code>gl_Position = ftransform();</code>.</p>

<p><strong>OpenGL 3</strong> completely deprecated the immediate mode fixed-function pipeline. Of course this introduced backwards-compatability issues so they also introduced several <em>profiles</em> to allow backward compatability to be optionally compiled in. The <em>compatability profile</em> can be requested to enable deprecated features, while the <em>core profile</em> disallows the use of these features.</p>

<p>Since OpenGL 2 however there have been no truly large architectural changes (unfortunately.) Changes have focused around adding features (new types of shaders, for example) to provide better performance and more generally useful aspects of existing functionality. This is a shame because there are still a lot of problems with OpenGL that many developers want addressed &ndash; the original goal of OpenGL 3 was to include massive refactorings to remove all the global state (more on this later) but vendors objected so strenuously that this work was put off until <em>GLNext</em> which is now known as <a href="https://www.khronos.org/vulkan">Vulkan</a>.</p>

<p>OpenGL shaders are written in their own language &ndash; <strong>GLSL</strong>. GLSL has its own varied syntax between versions, and to make things even more complicated they support the notion of extensions. The best bet is to decide on which version of OpenGL you will be learning and learn the GLSL appropriate to that version. They are all quite similar in form but are syntactically incompatible with each other.</p>

<p>Some mention should be made regarding extensions.  Extensions are often touted as a great feature in OpenGL not available in other graphics libraries such as DirectX. New commands and enumerations are often added to the OpenGL API by vendors as extensions, and then if this functionality proves popular it becomes formalised as part of the API in a later version.</p>

<p>Each OpenGL version has a known set of extensions. These can be browsed in the Khronos reference implementations &ndash; for example the OpenGL ES 2 extensions are in the <a href="https://www.khronos.org/registry/gles/api/GLES2/gl2ext.h">GLES2/gl2ext.h</a> header file. There are idiomatic ways to detect support for and load these extensions using system calls but most people use an <strong>extension loader</strong> of some type. A very popular extension loader is the <a href="http://glew.sourceforge.net/">OpenGL Extension Wrangler Library</a> (also known as GLEW.)</p>

<p>If you want some functionality not natively available in your chosen version of the OpenGL API you will often find an extension that provides that functionality. The problem is, extensions are not supported uniformly on all platforms with all drivers, so quite often you&rsquo;ll have to work around the missing functionality in some platform anyway. This severely limits the usefulness of extensions, and in general you should try to do without them if possible.</p>

<p>I personally do use a few extensions in a few circumstances: either to get around eccentricities of a particular platform (e.g., some Windows platforms use BGRA instead of RGBA framebuffer formats, which are only available through the GL_EXT_texture_format_BGRA8888 extension) or if I have through experimentation determined that an extension is very broadly supported.</p>

<p>Here&rsquo;s a quick summary of what I understand of the different OpenGL versions:</p>

<h3>OpenGL 1 &ndash; high level and slow but simple</h3>

<ul>
<li>Immediate mode only (fixed function pipeline)</li>
<li>no shaders</li>
<li>a lot of people still use this when demonstrating functionality</li>
<li>only version natively supported by Windows</li>
</ul>


<h3>OpenGL 2 &ndash; shaders run on the GPU</h3>

<ul>
<li>first shader-based version</li>
<li>vertex and fragment shaders</li>
<li>still have access to the fixed function pipeline &ndash; even within shaders</li>
</ul>


<h3>OpenGL 3 &ndash; a controversial release</h3>

<ul>
<li>framebuffers for rendering to non-screen targets (e.g., render to a texture)</li>
<li>vertex array objects (VAOs) allow great performance boosts by quickly binding and unbinding whole groups of buffers and attribute bindings</li>
<li>geometry shaders (modify/extend geometry of primitives)</li>
<li>significant (breaking!) changes to GLSL shader language</li>
<li>deprecated most 1.0 functionality (immediate mode stuff) introducing compatability and core modes

<ul>
<li>compatability mode gives access to the old fixed function pipeline</li>
<li>core mode does not</li>
</ul>
</li>
</ul>


<h3>OpenGL 4 &ndash; modernisation, performance and professional enhancements</h3>

<ul>
<li>OpenGL 4.0 goal was to achieve parity with Direct3D 11</li>
<li><a href="http://arstechnica.com/information-technology/2014/08/opengl-4-5-released-with-one-of-direct3ds-best-features/">OpenGL 4.5</a> goal was to achieve parity with Direct3D 12</li>
<li>tesselation shaders introduce extra polygons for &lsquo;denser&rsquo; meshes with smoother curves</li>
<li>compute shaders for using the GPU for non-graphics computations (GPGPU stuff)</li>
<li>GPGPU compute shader uses SPIR &ndash; an intermediate language based on LLVM</li>
<li>Direct State Access &ndash; mitigate long-standing architectural problems with immediate mode</li>
<li>modern OSX supports up to OpenGL 4.1</li>
</ul>


<h3>OpenGL ES &ndash; simplified for embedded systems</h3>

<ul>
<li>OpenGL ES 1.0 based on OpenGL 1.3</li>
<li>OpenGL ES 1.1 based on OpenGL 1.5</li>
<li>OpenGL ES 2.0 based on OpenGL 2.0

<ul>
<li>WebGL is based on OpenGL ES 2.0</li>
<li><a href="https://code.google.com/p/angleproject/">Google ANGLE project</a> provides OpenGL ES 2.0 support on Windows (wraps DirectX API)</li>
</ul>
</li>
<li>OpenGL ES 3.0 full subset of OpenGL 4.3

<ul>
<li>GLSL ES 3.0 based on GLSL 3.3</li>
<li>similar to OpenGL 3 but without geometry shaders</li>
<li>supported by modern iOS and Android devices</li>
<li><a href="https://code.google.com/p/angleproject/wiki/Update20150105">experimental support</a> in Google ANGLE project</li>
</ul>
</li>
</ul>


<h3>WebGL</h3>

<ul>
<li>based on OpenGL ES 2.0</li>
<li>I don&rsquo;t know much about this yet, but <a href="http://webglfundamentals.com">WebGL Fundamentals</a> is a great resource</li>
</ul>


<h3>Vulkan &ndash; GLNext, lots of hype but not much information yet</h3>

<ul>
<li>get away from immediate mode single-threaded global state context heritage</li>
<li>allow shaders to be written in a variety of languages</li>
<li><a href="http://arstechnica.com/gadgets/2015/03/khronos-unveils-vulkan-opengl-built-for-modern-systems/">http://arstechnica.com/gadgets/2015/03/khronos-unveils-vulkan-opengl-built-for-modern-systems/</a></li>
</ul>


<p>I have chosen to do most of my experimentation in OpenGL ES 2. This should give me the broadest platform availability as well as being compatible with WebGL for web demonstrations. I have resorted to using a few extensions that are supported on the platforms I use where necessary (e.g., to get anti aliasing), though I try to avoid this where possible.</p>

<h2>Challenges you will face</h2>

<ul>
<li>Having to learn support libraries in addition to the OpenGL API</li>
<li>Cross platform support is difficult as many features are not uniformly supported</li>
<li>Debugging GL state in different platforms &ndash; OpenGL debugging tools are not great, and there are none that work cross-platform</li>
<li>Multithreading correctly is extremely difficult &ndash; if possible just do all your rendering on one thread!</li>
<li>Managing extensions can be a pain in the ass</li>
<li>State management &ndash; OpenGL uses global state, so it is impossible to write optimal abstractions because cannot encapsulate state, and state querying is prohibitively expensive. So you end up redundantly setting state that has often already been set.</li>
<li>lots of problems: <a href="http://richg42.blogspot.jp/2014/05/things-that-drive-me-nuts-about-opengl.html">http://richg42.blogspot.jp/2014/05/things-that-drive-me-nuts-about-opengl.html</a></li>
</ul>


<h3>Documentation and tutorials</h3>

<p>As I mentioned there&rsquo;s not a lot of great documentation out there. I started to create <a href="http://cechner.github.io/">my own responsive OpenGL documentation</a> and then discovered that there&rsquo;s already a pretty sweet one out there called <a href="http://docs.gl/">docs.gl</a>. Docs.gl is great because it makes it clear which commands are supported in which versions of OpenGL &ndash; something that&rsquo;s hard to figure out from the more official sources.</p>

<p>A great beginner tutorial is <a href="http://open.gl/">open.gl</a> &ndash; it&rsquo;s modern, minimalistic and well written. A very similar-seeming tutorial series that goes into more advanced techniques is <a href="http://learnopengl.com/">Learn OpenGL</a>. There used to be a fantastic series called the ArcSynthesis tutorials but they seem to have died. There is a <a href="http://www.pdfiles.com/pdf/files/English/Designing_&amp;_Graphics/Learning_Modern_3D_Graphics_Programming.pdf">PDF version of their content</a> that seems pretty good though.</p>

<p><a href="http://ogldev.atspace.co.uk/index.html">OGLdev</a> is a series of tutorials that go into more depth than the open.gl site above, and is in bite-sized chunks for easier consumption.</p>

<p>If you want to step through some sample code, download the <a href="https://github.com/progschj/OpenGL-Examples">OpenGL-Examples</a> github repository. It seems pretty easy to use and goes into fairly advanced topics.</p>

<p>If you want <em>really really detailed</em> runthrough of the graphics pipeline, have a look at <a href="https://fgiesen.wordpress.com/category/graphics-pipeline/">the ryg blog</a>. I haven&rsquo;t made my way through it yet but I really want to. Fabien Gleson seems to be pretty knowledgable about not only low level rendering details but also general low level computing concepts in general.</p>

<p>Finally, if you&rsquo;re into WebGL you should check out Gregg Tavares' <a href="http://webglfundamentals.org/">WebGL Fundamentals</a>. Gregg has a lot of experience working on WebGL in Chrome and game programming in general and has made some fantastic javascript support libraries that make every-day WebGL much simpler.</p>

<p>As I mentioned before though, you can easily try reading the specs yourself. Google is your friend here &ndash; search for the specific version + &lsquo;spec&rsquo; and it will <a href="https://www.google.com.au/search?q=opengl+3+spec">usually be the first hit</a>.</p>

<h2>Debugging</h2>

<p>There are <a href="https://www.opengl.org/wiki/Debugging_Tools">many different OpenGL debugging tools</a> on different platforms and they are all generally pretty bad. I haven&rsquo;t spent a lot of time with any of them so please tell me if you find a good one!</p>

<p>If you&rsquo;re on OSX you can give the <a href="https://developer.apple.com/library/mac/documentation/GraphicsImaging/Conceptual/OpenGLProfilerUserGuide/Introduction/Introduction.html">OpenGL Profiler</a> a go &ndash; it allows you to set breakpoints on certain GL calls, look at frame buffers (though I found this difficult to do) and much more.</p>

<p>Windows users should check out <a href="https://github.com/baldurk/renderdoc">RenderDoc</a>, which allows you to track API calls, view render buffers and a lot of other stuff, for both OpenGL and DirectX. You can also invoke the DLL directly to dump various information from within your application. I haven&rsquo;t used it myself though so I won&rsquo;t go on further.</p>

<p>It&rsquo;s also a great idea to have some macros that optionally call <code>glGetError()</code> after every OpenGL call you make so you can easily track down where things begin to go awry. Unfortunately though querying the context can be fairly expensive so you should make it easy to disable this macro when things are not going awry.</p>

<p>Feel free to copy-paste this into your codebase:
``` c++
// #define DEBUG_OPENGL_COMMANDS // uncomment this to enable error checking</p>

<h1>ifdef DEBUG_OPENGL_COMMANDS</h1>

<p>  void checkOpenGLError(const char<em> function, const char</em> file, int line) {</p>

<pre><code>auto err = glGetError(); if (err == GL_NO_ERROR) return;
const char * err_msg = "unrecognised";
switch (err) {
  case GL_INVALID_ENUM: err_msg = "GL_INVALID_ENUM"; break;
  case GL_INVALID_VALUE: err_msg = "GL_INVALID_VALUE"; break;
  case GL_INVALID_OPERATION: err_msg = "GL_INVALID_OPERATION"; break;
  case GL_STACK_OVERFLOW: err_msg = "GL_STACK_OVERFLOW"; break;
  case GL_STACK_UNDERFLOW: err_msg = "GL_STACK_UNDERFLOW"; break;
  case GL_OUT_OF_MEMORY: err_msg = "GL_OUT_OF_MEMORY"; break;
  case GL_INVALID_FRAMEBUFFER_OPERATION: err_msg = "GL_INVALID_FRAMEBUFFER_OPERATION"; break;
  default:
}
// on Windows call ::OutputDebugString 
fprintf(stderr, "OpenGL error '%s' (0x%04x) called from %s in file %s line %d\n", 
  err_msg, err, function, file, line);
</code></pre>

<p>  }</p>

<h1>define GL_VERIFY(stmt) do { stmt; checkOpenGLError(#stmt, <strong>FUNCTION</strong>, <strong>FILE</strong>, <strong>LINE</strong>); } while (0)</h1>

<h1>define GL_CHECK() do { checkOpenGLError(<strong>FUNCTION</strong>, <strong>FILE</strong>, <strong>LINE</strong>); } while (0)</h1>

<h1>define GL_IGNORE(stmt) do { GL_CHECK(); stmt; glGetError(); } while (0)</h1>

<h1>else</h1>

<h1>define GL_VERIFY(stmt) stmt</h1>

<h1>define GL_CHECK()</h1>

<h1>define GL_IGNORE(stmt) stmt</h1>

<h1>endif</h1>

<p>```</p>

<p>Then you just wrap all your function calls like so: <code>GL_VERIFY(glDrawElements(...))</code>. If you want to just check for errors at a particular point in your code, just call <code>GL_CHECK()</code>.</p>

<p>One final interesting note about debugging OpenGL: If you are using a Google&rsquo;s ANGLE OpenGL driver that you compiled yourself you can step into it, so if you start getting vague sounding errors like <code>GL_INVALID_FRAMEBUFFER_OPERATION</code> but want to know specifically what the problem is, you can step into the ANGLE DLLs yourself to see which part of their validation fails. It&rsquo;s like running a own fully-compliant validation layer in your own client code.</p>

<h2>Upcoming</h2>

<p>Now that I&rsquo;ve gotten all the basic stuff out of the way I&rsquo;d like to go into a bunch of other more advanced things that I thought wasn&rsquo;t particularly easy to get help with. In no particular order:</p>

<ul>
<li>Pros and cons of writing an OpenGL wrapper library (<a href="https://github.com/seshbot/glpp">my glpp library</a>)</li>
<li>Building and using Google&rsquo;s ANGLE library</li>
<li>Using OpenGL for 2D:

<ul>
<li>setting up the GL context (depth buffer)</li>
<li>basic 2D coords</li>
<li>drawing primitives</li>
<li>simple texture</li>
<li>using orthographic projection with 2D</li>
</ul>
</li>
<li>Using OpenGL for 3D:

<ul>
<li>setting up the GL context (blending, face culling)</li>
<li>3d coordinate system (plus MVP, normal matrix)</li>
<li>drawing primitives</li>
<li>perspective projection and the frustum</li>
</ul>
</li>
<li>3D lighting</li>
<li>3D shadows</li>
<li>Particle systems</li>
<li>Texturing (textures, texture unit and samplers), sampling, blending, alpha discard, stencil testing, <a href="http://stackoverflow.com/questions/9224300/what-does-gltexstorage-do">glTexStorage</a></li>
<li>Multi pass rendering (using FrameBuffers):

<ul>
<li>Render scene &ndash;> FBO &ndash;> texture colour buffer &ndash;> screen rectangle &ndash;> post effect frag shader &ndash;> screen</li>
<li>Post processing (HSV and gamma correction)</li>
</ul>
</li>
<li>Loading models and animations (using assimp)</li>
<li>Rendering text (using stb)</li>
<li>Using Qt/QML with OpenGL</li>
<li>Object Picking in a 3D scene</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week in Review: Running in an infinite world]]></title>
    <link href="http://seshbot.com/blog/2014/05/31/week-in-review-running-in-an-infinite-world/"/>
    <updated>2014-05-31T19:24:19+08:00</updated>
    <id>http://seshbot.com/blog/2014/05/31/week-in-review-running-in-an-infinite-world</id>
    <content type="html"><![CDATA[<p>Here&rsquo;s a quick snap of the 2D SFML client alongside a new OpenGL client:
<img src="/images/upload/2014-05-31-compare.png" title="&ldquo;SFML client alongside OpenGL client with QML overlays&rdquo;" ></p>

<p>OK I could have spent a bit longer on the screenshot &ndash; I haven&rsquo;t made any models yet other than cubes, and I&rsquo;m not using any textures, and I should have adjusted the angle and colour of the directional light to make it look like evening or morning or something, to show off some of the immediate benefits of going to 3D.</p>

<p>I did spend some time making the camera zoom in from a map-like view to a chase-cam view:
<img src="/images/upload/2014-05-31-zoomed.png" title="&ldquo;The camera is tethered to the player character&rdquo;" ></p>

<p>In general however I&rsquo;m hoping to ramp-down on the OpenGL stuff and go back to working on the core functionality. The last week was actually pretty productive:
<img src="/images/upload/2014-05-31-bugfixes.png" title="&ldquo;Most recent git checkins&rdquo;" ></p>

<p>Also I&rsquo;ve been spending a fair amount of time in <a href="http://picopicocafe.com">PicoPico Cafe</a> when it&rsquo;s open, and I&rsquo;m guessing I&rsquo;ll be spending more time there in the coming weeks as a bunch of construction is scheduled outside my house during business hours when nobody is supposed to be home.</p>

<!-- more -->


<h2>World Building</h2>

<p>The 3D functionality in my game so far is provided by a thin layer that invokes the exact same service layer (game, player, mobile and prop unit services) used by the 2D client. I took the time to finally make some extensive refactorings that I&rsquo;ve been planning for some time:</p>

<h4>Infinte Terrain!</h4>

<p>The terrain now runs effectively infinitely, in that it can cache any combination of arbitrary regions. The complicating factor was the fact that I want the regions to be stored in a sensible way that provides for really fast lookups many times per second as entities roam around the countryside, even outside of the player&rsquo;s view. More work needs to be done on this to unload cached regions that are no longer relevant.</p>

<h4>Infinte Entities!</h4>

<p>The entity service, responsible for facilitating rapid interactions with many geographically-local entities, can now scale out exponentially more efficiently than before. Entity &lsquo;moment&rsquo; information is stored contiguously in memory for super cache friendly iteration. Unfortunately this encourages the programmer to pre-allocate large portions of memory to allow the number of entities to fluctuate without large re-allocations.</p>

<p>Because of this obsession with data locality, every region was previously allocating enough space for all entities in the game to coexist at once so I could trivially say <code>auto moment = entities_[entityId].moment;</code> (or something kinda like that.) Now I changed it so there is a single ledger shared outside the specific regions with lookup instructions for each entity indicating that entity&rsquo;s region and index within that region. This means that the regions can be much smaller, reducing the region-to-memory relationship from exponential to linear, which is HUGE. (Of course, the old scheme was never meant to stay in there.)</p>

<p>I now have 1,000,000 entities pre-allocated without straining memory or computational resources at all (about 100MB memory for the whole app) whereas before I was settling for about 10,000 lest I run into gigabytes for a game with a relatively small number of regions.</p>

<h4>Lots more stuff of course</h4>

<p>I also made an interesting change to the core &lsquo;entity moment&rsquo; structure so it includes a &lsquo;facing direction&rsquo; as well as the existing velocity vector that indicates which direction and at what speed the entity is moving. I was having trouble determining the direction an entity should face when it is not moving (the velocity is 0,0 so has no implicit direction.) This wasnt a problem when I was rendering all entities as circles.</p>

<h2>Back to OpenGL 2 again</h2>

<p>The more I learn about the troubles people have with OpenGL the more concerned I became about getting my app to work on Windows. Through a fine bit of serendipity I met the always helpful <a href="http://greggman.com/">Greggman</a> at PicoPico, who was one of the devs who implemented WebGL in Google Chrome. He gave me some great advice that lined up with a few things I&rsquo;d found on the internet and convinced me to move to OpenGL ES 2.0.</p>

<p>As I mentioned last week OpenGL support in Windows is lacking due to MS pushing their own DirectX 3D stack. But fortunately Google have written a library called <a href="https://code.google.com/p/angleproject/">ANGLE</a> that is a &lsquo;conformant implementation of the OpenGL ES 2.0 specification that is hardware‐accelerated via Direct3D.&rsquo; This means that if you write conformant OpenGL ES 2.0 code you get DirectX compliance for free (at least that is the idea.)</p>

<p>Another advantage of OpenGL ES (other than the widespread support, including on mobile devices) is that it is much more stringently standardised. Apparently standard OpenGL does not come with conformance tests of any kind, so vendors don&rsquo;t have a mechanical way of verifying that they have implemented conformant drivers. ES does have such tests however so anecdotally provides a much cleaner integration experience.</p>

<p>Of course I&rsquo;m effectively going back to OpenGL 2 syntax and functionality which is a bit of a drag, but I wasn&rsquo;t really using geometry shaders or complex alternative rendering pipelines anyway.</p>

<h2>Object Picking with the mouse</h2>

<p>If someone clicks on a pixel on the screen (in pixel coordinates), how do you discover which object they were clicking on? This is deceptively difficult! The old way of doing it involved using a special part of OpenGL called <em>selection buffers</em> that allows you to assign a name to each entity being rendered, and query a pixel for the name of the entity rendered therein. That is deprecated however.</p>

<p>The modern analog of this is a manual process called <em>colour picking</em>, which involves rendering identifying values instead of colours to a special buffer that is not rendered. This image can be queried via <code>glReadPixels()</code>, the returned result is the identifying value of the object at that pixel. I chose not to go down this route though.</p>

<p>Instead, I chose to do it entirely on the CPU by performing the rendering calculations in reverse, in a process often known as <em>raycasting</em>. The rendering process involves taking each entity in your scene and multiply its coordinates by a <em>model matrix</em>, then a <em>view matrix</em> and then a <em>projection matrix</em>, transforming it into world space, then into camera space, and then into screen space. To figure out what object is at a pixel, I perform these operations in reverse by passing those pixel coordinates to the <em>inverted</em> perspective matrix and then to an <em>inverted</em> view matrix. This gives you the location of a point in
space corresponding to that point on the screen. If you draw a line from the location of the camera through that point, you have a <em>ray</em> into 3D space that is every point behind that pixel! The object picked is the object intersecting with that ray that is closest to the camera (I&rsquo;m just iterating through every object near the camera for now.)</p>

<p>For more information on this have a look at <a href="http://antongerdelan.net/opengl/raycasting.html">this article on raycasting</a>. I like this because it is purely algorithmic and doesn&rsquo;t require special shader code or anything.</p>

<h2>Qt Quick and OpenGL</h2>

<p>I continue to be happy with Qt &ndash; they are so active, they&rsquo;re always releasing interesting new tech. It is hard to catch them for feedback but it can be done and when they are on IRC they are very helpful.</p>

<p>Unfortunately the OpenGL support in Qt Quick is still in a great state of flux, so finding the correct way to use OpenGL ES was quite tricky. Turns out however that simply <em>not</em> specifying an OpenGL version, and using the default function pointers their <a href="http://qt-project.org/doc/qt-5/qopenglfunctions.html">utility library provides</a> is the proper way of using OpenGL ES in a portable fashion, even when your platform does not provide it (it falls back on the &lsquo;desktop&rsquo; 2.0 functions.)</p>

<h2>Up Next</h2>

<p>My representation of the world in the core code is still effectively 2D in that all entities have an <em>x</em> and a <em>y</em> but no altitude component. For rendering purposes that is extracted from the terrain service by the rendering engine for visual effect only. But if I am to allow jumping and have some notion of visual occlusion I will soon have to add a <em>z</em> component to the velocity and the position of each entity. This will also mean that the entity service will need to be dependent on the
terrain service.</p>

<p>I also want to implement a proper day/night cycle and some fancy controls in the GUI, like a slider that allows you to choose the time of day. This will hopefully provide a large ambiance improvement at little cost.</p>

<p>As far as blogging goes, last week I was intending my next post to be about my adventures in OpenGL and to provide a clear roadmap for others who want to follow the same path I took, but it is such a <em>huge</em> topic, and I am still not certain I have all the facts right, that I have put it off so far. We&rsquo;ll see how that goes&hellip;</p>
]]></content>
  </entry>
  
</feed>
