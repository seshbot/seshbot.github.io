<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: HowTo | Seshbot Programs]]></title>
  <link href="http://seshbot.com/blog/categories/howto/atom.xml" rel="self"/>
  <link href="http://seshbot.com/"/>
  <updated>2015-09-22T05:28:06+00:00</updated>
  <id>http://seshbot.com/</id>
  <author>
    <name><![CDATA[Paul Cechner]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[An Introduction to OpenGL - Getting Started]]></title>
    <link href="http://seshbot.com/blog/2015/05/05/an-introduction-to-opengl-getting-started/"/>
    <updated>2015-05-05T07:40:37+00:00</updated>
    <id>http://seshbot.com/blog/2015/05/05/an-introduction-to-opengl-getting-started</id>
    <content type="html"><![CDATA[<p><em>This article is a culmination of all the little notes I took while learning OpenGL over the last several months. It&rsquo;s mostly stuff that I found difficult to research plus a little summary of the differences between OpenGL versions.</em></p>

<p><em>What a daunting task!</em></p>

<p><em>If you have any recommendations on how this could be more beginner-friendly please tell me.</em></p>

<p><em>Also thanks <a href="http://greggman.com/">Gregg Tavares</a> for pointing out my various errors!</em></p>

<h2>Things I wish I knew when learning OpenGL</h2>

<p>The most important thing a programmer should know before deciding whether to learn OpenGL is that OpenGL is very low level, poorly documented and extremely crufty. This is because it is an API specification and not a product library per-se. It is up to the many various vendors to implement the API spec as best they can.</p>

<p>Surround your pullquote like this {" text to be quoted "}</p>

<p>The next thing to know about modern OpenGL is that these days it does very little legwork for you other than allowing you to run a program on the GPU. You will have to write the GPU shader programs that do everything from transforming your own application data into screen-space coordinates, to calculating the exact colour of every pixel on the screen incorporating lighting and shading algorithms that you implement yourself (fortunately linear algebra makes this stuff a lot simpler than it sounds!) So OpenGL will not do any inherently 3D stuff for you &ndash; most OpenGL commands and types are capable of describing 3D positions, directions and transformations but you have to do the grunt work yourself.</p>

<p>The third immediate concern &ndash; <em>OpenGL does not work out of the box</em>! An annoying truth is that OpenGL realistically requires supporting libraries in order to function, most importantly to create a context within which the rendering operations can work. It is very common to incorporating at least three libraries &ndash; one to generate a GL context into which you render, a matrix and vector manipulation library, and an extension loader for when you need a little more functionality than your platform provides.</p>

<p>Surround your pullquote like this {" text to be quoted "}</p>

<p>If you are only targeting Windows you might consider DirectX. If you don&rsquo;t need to interact directly with your shaders, and are happy to work at a higher level of abstraction and not with the GPU directly, perhaps a higher level graphics library such as Unity or UDK would work better for you.</p>

<p>So assuming you still want to start using OpenGL, this article might be helpful to you. My intention is to mention a lot of stuff I had to hunt around for that seemed pretty important to me while I was trying to learn it myself. I will not be doing a step-by-step guide to performing specific OpenGL tasks however &ndash; for a good getting started guide check out <a href="https://open.gl/">open.gl</a> which is both modern and easy to follow.</p>

<p>To use OpenGL effectively I figure you&rsquo;d need to understand:</p>

<ul>
<li>opening an OpenGL window (i.e., creating a context)</li>
<li>the basics of rendering:

<ul>
<li>primitives, vertices and fragments</li>
<li>coordinate systems:

<ul>
<li>built-in normalised device coordinates (NDC) and clip coordinates</li>
<li>3D model, view and perspective coordinates</li>
</ul>
</li>
<li>shaders and the render pipeline (how data gets from your app to the screen):

<ul>
<li>vertex and fragment shaders</li>
<li>passing uniforms and attributes into the pipeline</li>
<li>vertex buffers (VBOs)</li>
<li>passing varying data from the vertex shader to the fragment shader</li>
</ul>
</li>
<li>the fixed-function pipeline (now deprecated)</li>
</ul>
</li>
<li>linear algebra (the magical language of graphics programming)</li>
<li>a rundown on all the different major OpenGL versions</li>
<li>major challenges that you certainly will face moving forward</li>
</ul>


<p>I&rsquo;ll leave more advanced core concepts such as framebuffers and textures for a later article.</p>

<!-- more -->


<h2>Starting a new project</h2>

<p>I suppose this is the most important thing to a lot of people, so I&rsquo;ll show how I bootstrap a new OpenGL project. I haven&rsquo;t been at it long so take it with a grain of salt, but I tried to focus on building a cross-platform solution.</p>

<p>I generally depend on four libraries. Although technically you could get away without any supporting libraries, these save a lot of time and effort. The libraries are:</p>

<ul>
<li>Google&rsquo;s <a href="https://code.google.com/p/angleproject/">ANGLE project</a> provides an OpenGL ES 2.0 (soon 3.0) driver library for Windows that wraps Direct3D. This is useful so you don&rsquo;t have to depend on the user downloading the OpenGL drivers for their graphics card on Windows.</li>
<li><a href="glfw.org">GLFW</a> to create a window and otherwise interact with the OS and other devices. Many people prefer <a href="https://www.libsdl.org/">SDL2</a> or <a href="https://www.opengl.org/resources/libraries/glut/">GLUT</a>. Alternatively you could use the standard low-level supporting libraries supported by your operating system &ndash; WGL, GLX or EGL.</li>
<li><a href="glm.g-truc.net">GLM</a> is a widely used vector and matrix library. It&rsquo;s particularly nice because it mirrors GLSL standard types and operations as much as possible, so hopefully there&rsquo;s some learning synergies there. I have tried using <a href="http://eigen.tuxfamily.org/">Eigen</a> which is more of a general linear algebra library focusing on performance, but it has a lot of limitations on how you can use (passing or storing types by value is complicated) it because it uses low-level processor vector operations under the covers. Of course you could always write your own matrix classes, but it&rsquo;s a pretty big task.</li>
<li><a href="http://glew.sourceforge.net/">GLEW</a> is a commonly used extension loader. Unfortunately extension loaders in general don&rsquo;t seem to work with ANGLE so I haven&rsquo;t used it much. <a href="https://github.com/hpicgs/glbinding">glbinding</a> and <a href="https://bitbucket.org/alfonse/glloadgen/wiki/Home">glLoadGen</a> are both code generators that create loaders for specific target versions of OpenGL. These don&rsquo;t seem to be able to target OpenGL ES versions however.</li>
</ul>


<p>I have created a <a href="https://github.com/seshbot/new-gl-app">simple GL application on GitHub</a> that I intended to be used as a starting point for OpenGL ES 2 experimentation. It should work on Windows, Linux and OSX, and the only external dependency should be CMake which is pretty easy to install. Then, hopefully, getting it running is a matter of (depending on your platform of choice):</p>

<p>``` bash
git clone <a href="https://github.com/seshbot/new-gl-app">https://github.com/seshbot/new-gl-app</a>
cd new-gl-app
mkdir build &amp;&amp; cd build
cmake ..</p>

<p>./glapp
```</p>

<p>Alternatively you could try copying <a href="/assets/2015-05-13-gl1.html">my sample GL HTML page</a> and copy the <a href="/assets/js/webgl-utils.js">webgl-utils.js</a> file into a subdirectory called &lsquo;js&rsquo; under that. Run the HTML file in your browser and you&rsquo;ll have a WebGL app!</p>

<h2>Understanding the OpenGL API</h2>

<p><em>Note:</em> I&rsquo;m using OpenGL ES 2 GLSL syntax in my examples because I believe that&rsquo;s probably got the broadest platform support, and is most similar to WebGL. The concepts are the same for later versions, aside from the syntactic differences. As I am focusing on explaining core concepts only OpenGL ES 2 should be fine for my purposes.</p>

<p>Take a moment to read some OpenGL specifications &ndash; they are probably easier to understand than you&rsquo;d think. Here&rsquo;s the <a href="https://www.khronos.org/registry/gles/specs/2.0/es_full_spec_2.0.25.pdf">OpenGL ES 2.0 Spec</a> if you want a definitive source for all this stuff.</p>

<p>I will probably mention the Khronos Group a lot throughout this article. The <a href="http://khronos.org">Khronos Group</a> is a consortium of companies such as Sun Microsystems, NVidia and Silicon Graphics who work on standardising graphics APIs, including OpenGL. Part of their OpenGL standardisation process is to provide reference interfaces (header files) for each version of the API that vendor implementations should conform to.</p>

<p>Here&rsquo;s a bit of a glossary of terms and concepts that are necessary to become familiar with in order to be an effective OpenGL programmer.</p>

<h3>The OpenGL API &ndash; commands, enums and objects</h3>

<p>The OpenGL API consists entirely of commands (e.g., <code>glDrawElements()</code>), enums (e.g., <code>GL_COLOR_BUFFER_BIT</code>) and, conceptually, objects.</p>

<p>The <strong>commands</strong> and <strong>enums</strong> are described in the specification but can also be browsed on the Khronos API header files &ndash; have a look at <a href="https://www.khronos.org/registry/gles/api/GLES2/gl2.h">GLES2/gl2.h</a> for the standard GLES2 header. Most vendor implementations use these exact files as the entry point for their libraries.</p>

<p>OpenGL <strong>objects</strong> are the conceptual entities you create using the API, such as vertex buffers (VBOs), vertex arrays (VAOs), shaders, programs, textures and framebuffers. OpenGL has commands to create, bind and delete each of the different types of objects. Note that the objects you create are only valid with the context that was active when you created them!</p>

<p>OpenGL commands that operate on OpenGL objects require those objects to be <em>bound</em>. For example when you call <code>glDrawElements()</code> you don&rsquo;t pass the program, the target framebuffer or the vertex or index buffer IDs to the function, it just operates on whichever program, framebuffer and vertex buffers are currently bound (through the <code>glUseProgram()</code>, <code>glBindFramebuffer()</code> and <code>glBindBuffer()</code> commands.) Many people feel this is a confusing and error-prone way to do things; it means that if you call some library or function that uses OpenGL there is no way to determine whether it has modified the current context in any way, so you have to re-bind all your stuff. It&rsquo;s unfortunate but that&rsquo;s the API we are stuck with :(</p>

<p><em>Note:</em> it is interesting to note that the <code>glCreate</code>-type functions don&rsquo;t actually create or allocate anything! They give you back a free handle, but that handle isn&rsquo;t actually allocated until it is bound. This means that technically you&rsquo;re free to keep track of these IDs yourself and allocate them according to whatever scheme you most desire, though I&rsquo;m not sure why you&rsquo;d do that.</p>

<h3>OpenGL context</h3>

<p><em>Here I&rsquo;ll describe the concepts involved in creating and using OpenGL contexts. For a more tutorial-type approach have a look at <a href="https://open.gl/context">this guide</a> that shows how to use several different popular context management libraries.</em></p>

<p>The <strong>context</strong> encapsulates the rendering view, its rendering settings and which OpenGL <em>objects</em> are currently active (such as which shader will be invoked during rendering). When starting your application you will need to create a context and set the capabilities you want the driver will use while rendering, such as whether the renderer will perform scissor, stencil, depth or alpha testing, how the renderer will blend fragments into final pixel colours and what part of the window to render into (the viewport). Contexts are not thread-safe (unfortunately!) and are essentially global in scope (even more unfortunately!) and are the cause of most of the grief people have with OpenGL.</p>

<p>You should avoid querying the state of the context too much if possible (i.e., querying information about bound objects, or even constantly querying error state) &ndash; it is apparently quite slow. You should also avoid accessing context from multiple different threads. I believe you are supposed to create separate contexts in each thread you wish to render from, or more preferably not render from multiple threads at all.</p>

<p>It is also possible to create a <em>shared context</em> where objects created on one context are available in another. This can be tricky so proceed with caution &ndash; for example, according to the spec shared contexts may not share framebuffer objects for some reason. I found that out <a href="https://code.google.com/p/angleproject/issues/detail?id=979">the hard way</a>.</p>

<p>Unfortunately the creation of an OpenGL context is not defined by the OpenGL spec. If you want to create one you&rsquo;ll either have to look up how to do this on your platform of choice (for example Windows provides WGL, while <a href="http://www.geeks3d.com/20121109/overview-of-opengl-support-on-os-x/">OSX</a> and Linux systems use GLX) or use another third-party library that does this for you (I like GLFW, but SDL2 is very popular, and some people still use the older GLUT.) These libraries often give you access to keyboard and mouse input as well as various other utilities you might need when making your app (such as buffer swapping, text, audio and the like.)</p>

<p>Below is a simple example of creating a context using GLFW in C or C++. For a line-by-line explanation of this see <a href="http://www.glfw.org/docs/latest/quick.html">the official GLFW docs</a>.</p>

<p>``` c++ Creating a context with GLFW</p>

<h1>include &lt;GLFW/glfw3.h></h1>

<p>static void key_callback(GLFWwindow* window, int key, int scancode, int action, int mods) {</p>

<pre><code>if (key == GLFW_KEY_ESCAPE &amp;&amp; action == GLFW_PRESS)
    glfwSetWindowShouldClose(window, GL_TRUE);
</code></pre>

<p>}</p>

<p>int main() {
  if (!glfwInit())</p>

<pre><code>  exit(EXIT_FAILURE);
</code></pre>

<p>  // create context (unfortunately GLFW bundles this in with window creation)
  GLFWwindow* window = glfwCreateWindow(640, 480, &ldquo;Simple example&rdquo;, NULL, NULL);
  if (!window) {</p>

<pre><code>glfwTerminate();
exit(EXIT_FAILURE);
</code></pre>

<p>  }
  glfwMakeContextCurrent(window);
  glfwSwapInterval(1); // wait for a vsync before swapping to avoid &lsquo;tearing&rsquo;</p>

<p>  // tell GLFW to notify us when keys are pressed (esc will exit)
  glfwSetKeyCallback(window, key_callback);</p>

<p>  /////
  // OpenGL configure context capabilities
  glClearColor(1., 0., 0., 1.);
  glEnable(GL_DEPTH_TEST);
  glEnable(GL_BLEND);
  glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);</p>

<p>  // TODO: create OpenGL objects (shader programs, vertex buffers, etc) here
  ///</p>

<p>  // main loop
  while (!glfwWindowShouldClose(window)) {</p>

<pre><code>int width, height;
glfwGetFramebufferSize(window, &amp;width, &amp;height);

/////
// OpenGL render
glViewport(0, 0, width, height);
glClear(GL_COLOR_BUFFER_BIT);

// TODO: draw your primitives here!
///

glfwSwapBuffers(window);
</code></pre>

<p>  }</p>

<p>  // TODO: destroy OpenGL objects here</p>

<p>  glfwDestroyWindow(window);
  glfwTerminate();</p>

<p>  exit(EXIT_SUCCESS);
}
```
Most OpenGL context management libraries are very similar in usage to this.</p>

<h3>Primitives, Vertices and Fragments</h3>

<p>Each time you call an OpenGL drawing function you are drawing a <strong>primitive</strong>. You can think of a primitive as a shape of some kind that can be either 2D or 3D and rendered as a collection of points, lines or triangles.</p>

<p>You specify the primitive type you want to draw when invoking <code>glDrawElements()</code> or <code>glDrawArrays()</code>. Among the valid types are:</p>

<ul>
<li><code>GL_TRIANGLES</code>, which uses each 3 vectors to create a triangle. If you provide 9 vertices you will draw 3 triangles.</li>
<li><code>GL_TRIANGLE_STRIP</code> is more complicated in that it will use each set of 3 consecutive vertices to draw a triangle. In other words if you supply 4 vertices it will draw 2 triangles &ndash; one from v0, v1 and v2, another from v2, v1, v3. Notice that the first two vertices of every second triangle are re-ordered &ndash; this is to ensure that the triangles all face the same direction, as triangle direction is derived from which direction the triangle&rsquo;s vertices appear to be counter-clockwise in order (relevant if face culling is enabled.)</li>
<li><code>GL_POINTS</code>, which renders unconnected dots. You can control the size of the points by calling the <code>glPointSize()</code> command in desktop GL or by setting the <code>gl_PointSize</code> GLSL variable in OpenGL ES. You can also apply a texture to the point to render more complex particles (the GPU passes your fragment a <code>gl_PointCoord</code> variable to indicate which texture UV coordinates you should render.)</li>
<li><code>GL_LINES</code> is useful for quickly drawing a wireframe of your model.</li>
<li>there are also <code>GL_LINE_STRIP</code>, <code>GL_LINE_LOOP</code>, <code>GL_TRIANGLE_FAN</code>, <code>GL_QUADS</code>, <code>GL_QUAD_STRIP</code> and <code>GL_POLYGON</code>.</li>
</ul>


<p>You will describe your primitives as collections of <strong>vertices</strong>. These vertices are passed to the GPU and it then transforms them to triangles then <em>rasterises</em> those triangles into <strong>fragments</strong>. These fragments may be filtered, blended and anti-aliased and ultimately may be drawn as pixels on your screen. So technically <em>fragments are not pixels</em>.</p>

<h3>Coordinate systems</h3>

<p>The OpenGL <strong>coordinate system</strong> is simple but requires some explanation &ndash; put simply, the range of visible coordinates within the viewport goes from -1.0 to 1.0 in the X, Y and Z directions. This coordinate space is known as <strong>normalized device coordinates</strong> or NDC, and anything falling outside of this range will not be rendered. The X and Y coordinates describe the horizontal and vertical component of the pixel (-1, -1 corresponds with the bottom left corner of the viewport) and the Z axis is the <em>depth</em> component that is used for depth testing (if enabled.) By default the NDC Z coordinates move <em>away from</em> the viewer, so +Z is into the screen.</p>

<p><img class="center" src="/images/upload/2015-05-09-gl_1_ndc.png" title="&ldquo;Normalised device coordinates (NDC). By convention XYZ shown as RGB&rdquo;" ></p>

<p>But <strong>you won&rsquo;t be using NDC directly</strong> &ndash; you will be rendering your primitives in what is known as <em>clip coordinates</em>, which are <em>very</em> similar to NDC except with a 4th dimension (x, y, z and w). You might be somewhat confused when you write your vertex shader, when setting the mandatory <code>gl_Position</code> variable with the vertex coordinates (described later) you&rsquo;ll notice it is a <code>vec4</code>. What&rsquo;s the 4th dimension for? It turns out the 4th dimension <em>w</em> is used for <em>perspective division</em>, which is super useful for 3D graphics but useless for 2D. For now you just have to know that when the GPU transforms from <em>clip coordinates</em> to <em>NDC</em> it calculates something like this: <code>ndc_coords = clip_coords.xyz / clip_coords.w</code>. So if rendering 2D stuff to the screen just set <code>gl_Position.w</code> to 1.0.</p>

<p>When programming in 2D you may choose to draw all your primitives using these NDC coordinates to avoid having to convert between coordinate spaces at all. The samples in this article draw primitives using normalised device coordinates directly.</p>

<p>3D coordinate systems are more complicated and I will discuss them in depth in a later article. For now it is interesting to note two things: first, by convention OpenGL coordinate systems other than NDC generally have the Z axis moving <em>towards</em> the viewer, so the <em>negative</em> Z axis goes into the screen.</p>

<p>Secondly, when rendering 3D primitives the verticies that describe the primitive are usually transformed in between a well defined sequence of coordinate spaces. The coordinates in the model&rsquo;s local <strong>model space</strong> are first moved to <strong>world space</strong> where their position and orientation is given relative to all other objects in a scene (the same model may be used many times in the one scene, but each will look different if they have different model space transformations.) Then the coordinates are transformed to <strong>view space</strong> where their position and orientation are relative to the viewer&rsquo;s eye. Then they are transformed into clip coordinates and finally NDC as before. I hope to go into more detail on this process in a later article &ndash; I just wanted to list the terms here for completeness.</p>

<p>One final thing I will mention about 3D coordinates is that it is common to specify 3D positions, directions and transformations using 4 dimensional vectors and matricies. I won&rsquo;t go into it now but it is very useful to remember that <em>positional</em> coordinates generally have a 4th dimension <em>w</em> set to 1.0 and <em>directions</em> generally have <em>w</em> set to 0. This makes the linear algebra work out nicely when multiplying against transformation matricies as the 4th dimension in the vector usually controls the translation factor of the transformation, which is not relevant for directions (i.e., a &lsquo;north&rsquo; pointing vector is stil pointing north no matter where the vector is located in space.)</p>

<h3>Shaders and the render pipeline</h3>

<p>Every time you call an OpenGL draw operation (the <code>glDrawArrays()</code> or <code>glDrawElements()</code> commands) you invoke the entire render pipeline. This is when the GPU passes the verticies in your primitives to your vertex shader, clips the resulting coordinates, generates triangles from the vertices, rasterises them into fragments, passes those fragments to your fragment shader then tests visibility, blends and dithers those fragments into pixels on your screen (or some other framebuffer.)</p>

<p>The difference between <code>glDrawArrays()</code> and <code>glDrawElements()</code> is in how the GPU knows which vertices to use when transforming your vertices into triangles. <code>glDrawArrays()</code> is simpler in that it builds triangles using the vertices in their given order &ndash; in a GL_TRIANGLES primitive, triangle T0 will be built from vertices V0, V1 and V2, triangle T1 built from V3, V4, V5 and so on.</p>

<p><code>glDrawElements()</code> is more complex but often more performant. Instead of using the vertex data strictly in the order you declared them, it uses a separate buffer called the <em>index buffer</em> (also known as the <em>element buffer</em>) to determine which vertices to use in which triangles. This is great because it allows you to reuse vertices to create multiple triangles &ndash; an index buffer of <code>[0, 1, 2, 2, 1, 3, 1, 0, 4]</code> will construct 3 triangles from only 5 vertices! The first triangle will be using v0, v1 and v2, the second using v2, v1, v3 and the third using v1, v0 and v4.</p>

<p><em>Note:</em> Vertex buffers are bound by invoking <code>glBindBuffer(GL_ARRAY_BUFFER, my_buff_loc)</code> and index buffers are bound with <code>glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, my_buff_loc)</code>. Don&rsquo;t ask me about the names.</p>

<h4>Pipeline summary</h4>

<p>To draw a primitive, the GPU first needs your <em>vertex data</em>. The GPU will decode your vertex data to extract <em>vertex attributes</em> and pass those into your <em>vertex shader</em> once for each vertex. Your vertex shader is expected to output clip coordinates for that vertex so the GPU knows where on the screen to show it (if at all,) and information that the GPU should pass on to your fragment shader for fragments derived from this vertex.</p>

<p>Once the GPU has processed all vertices the GPU can clip the vertices to within the NDC area only, it transforms those vertices into triangles and then <em>rasterises</em> the triangles into fragments which sort-of represent the pixels of the primitive being drawn. The GPU will then call your <em>fragment shader</em> for each fragment, passing it the relevant output data of your <em>vertex shader</em>. Your fragment shader is expected to output a colour and optionally a new depth-value for the fragment.</p>

<p>The GPU then processes, blends and dithers the fragments to the output framebuffer, as described under <em>how the GPU writes fragments to the framebuffer</em> a few pages down.</p>

<p>You invoke the rendering pipeline by calling either the <code>glDrawArrays()</code> or <code>glDrawElements()</code> commands (or one of their variations.) Unless you&rsquo;re using the fixed function pipeline you&rsquo;ll have to have a shader program bound so the GPU knows which vertex shader and fragment shader to invoke. You will also have to specify the vertex data (the vertex positions and perhaps other information such as the vertex normals and colours) of the primitive you are rendering. The &lsquo;basic shader program&rsquo; a few pages down shows one way to do both of these things.</p>

<p><center><img src='/images/plantuml/c5680b261dbef74198e842be690657ca.png'></center></p>

<p>Once again, if you want more details on how this works have a look at the specs, like the <a href="https://www.khronos.org/registry/gles/specs/2.0/es_full_spec_2.0.25.pdf">OpenGL ES 2.0 Spec</a>.</p>

<h4>Passing data to the GPU &ndash; uniforms, attributes and varyings</h4>

<p>There are two ways your application can pass data to your shaders &ndash; <em>uniforms</em> that are set only once per <code>glDraw*</code> call and and <em>attributes</em> that may be different per-vertex. In addition, fragment shaders receive per-fragment input derived from the output of the vertex shader in <em>varyings</em> (see the <em>rendering pipeline</em> diagram above.)</p>

<h5>Sending uniform data to the GPU</h5>

<p>A <em>uniform</em> represents a variable that remains the same for the rendering of an entire primitive. This might be something like the object material or the position of the sun. The pattern for setting a uniform is to first get a handle to the uniform with <code>glGetUniformLocation(my_program, "my_uniform")</code>. Pass this handle to the <code>glUniform*</code> functions to set the uniform (for example <code>glUniform3f()</code> will set a uniform of type <code>vec3</code> in your shader.) Setting a uniform will make it available in any of the shaders in the bound program that are want to use it.</p>

<p>In the code example a few pages down you can see how the unifrom data is bound and updated:
<code>c++ uniform updating snippet
  auto time_loc = glGetUniformLocation(shaderProgram, "time");
  glUniform1f(time_loc, glfwGetTime());
</code></p>

<p>Uniforms may also be structures and arrays &ndash; the syntax to declare and use structs and arrays in GLSL (the shader code) is very similar to C, but setting them from the client application is a little tricky.</p>

<p><em>To set uniform values in a structure</em>: you essentially treat the variable as if it is in the namespace of the structure name. E.g., if the shader contains the code <code>struct Point { float x; float y; }; uniform Point p1;</code> you can access <code>p1.x</code> with exactly that syntax: <code>auto u_p1x = glGetUniformLocation(my_program, "p1.x")</code>.</p>

<p><em>To set uniform values in an array</em>: you access the specific array element using standard C syntax. If the shader contains the code <code>float xs[10];</code> you can get the location of a particluar element of <code>xs</code> with <code>auto u_xs = glGetUniformLocation(my_program, "xs[0]")</code>. You can use this uniform location to either set a single element using <code>glUniform*()</code> or set a number of the elements using <code>glUniform*v()</code>.</p>

<p><em>Note:</em> that you cannot apply offsets to the returned uniform location to access specific array elements &ndash; in the above example, I cannot call <code>glUniform1f(u_xs + 2, 1.)</code> as that could be the location of a totally different uniform. In this case you would either have to find the location of the element you want to access directly (in this case <code>glGetUniformLocation(my_program, "xs[2]")</code>) or set multiple elements (using <code>glUniform*v()</code>) in the array starting from the index we retrieved.</p>

<h5>Sending vertex attribute data to the GPU</h5>

<p>An <em>attribute</em> represents data related to the current vertex being processed. You specify where the GPU can find vertex data by calling the <code>glVertexAttribPointer*</code> functions for each vertex attribute in your vertex shader. This process is <em>very</em> different to specifying uniforms!</p>

<p>The standard form for using a vertex attribute is something like:
``` c++ setting vertex attribute data
// &hellip; first bind the appropriate shader</p>

<p>// during initialisation
GLint position_loc = glGetAttribLocation(shaderProgram, &ldquo;position&rdquo;);
glEnableVertexAttribArray(position_loc);
const float vertex_buffer[] = { 0., 0., 0., &hellip; }; // x, y, z positions
glVertexAttribPointer(position_loc, 3, GL_FLOAT, false, 0, vertex_buffer);
glDisableVertexAttribArray(position_loc);</p>

<p>// at render time
glEnableVertexAttribArray(position_loc);
glDrawArrays(GL_TRIANGLES, 0, 4); // draw verts 0..4 as triangles
glDisableVertexAttribArray(position_loc);
<code>``
The above code snippet illustrates how to passing a raw pointer to your vertex data to the GPU. This is not the usual case, because quite often your buffered data will interleave more than one attribute's worth of information (such as normals, vertex colours etc.) Usually you will create a __vertex buffer object__ (VBO) and use</code>glVertexAttribPointer()` to dictate how the GPU should extract the vertex info from that.</p>

<p><em>vertex buffer objects</em> are created, bound and destroyed like any other OpenGL object. You specify the data to buffer by calling the <code>glBindBuffer()</code> command with a pointer to the buffer. While this buffer is bound, any calls to <code>glVertexAttribPointer()</code> with a non-pointer in the final parameter will implicitly be referring to the bound buffer, using the final parameter instead as an offset into the buffer where the data may be found. This is necessary for interleaving vertex data in the same buffer.</p>

<p>Having a single buffer with different types of vertex information interleaved is very common. Your two tools for describing how this buffer data is formatted are the above-mentioned <em>offset</em> parameter and the <em>stride</em> parameter. While the <em>offset</em> describes the starting byte of an attribute in the buffer, the <em>stride</em> describes the total distance (in bytes!) between the start of that attribute and the start of the next instance of that attribute. A stride of <code>0</code> is considered special &ndash; it is used if the attribute is &lsquo;tightly packed&rsquo;, meaning the buffer contains only that attribute with no space between them.</p>

<p>This is best illustrated with an example:
``` c++ specifying attributes in an interleaved buffer</p>

<pre><code>float positions_and_colours_buffer[] = {
  -1., 0.,   1., 0., 0.,   // x, y,   r, g, b
  -1., -1.,  0., 1., 0.,   // x, y,   r, g, b
  // ...
};
glBindBuffer(GL_ARRAY_BUFFER, positions_and_colours_buffer);

auto position_offset = 0U;  // positions start 0 bytes in
auto colour_offset = 2 * sizeof(GLfloat); // normals start 8 bytes in
int stride = 5 * sizeof(GLfloat);  // each vertex is total 10 bytes

glVertexAttribPointer(position_loc, 3, GL_FLOAT, false, stride, (void*)position_offset);  
glVertexAtrirbPointer(colour_loc, 3, GL_FLOAT, false, stride, (void*)colour_offset);
</code></pre>

<p>```</p>

<p><em>Note</em>: in OpenGL 3.0 and above you will want to use <strong>vertex array objects</strong> (VAOs) to speed up your rendering process. A <em>VAO</em> offers you shorthand for binding vertex bufffers and the related vertex attributes for those buffers. This means that <code>glBindVertexArray()</code> can replace a number of <code>glBindBuffer()</code> and <code>glEnableVertexAttribArray()</code> commands.</p>

<p>The pattern for using this is:
``` c++
// at init time when creating buffers
glBindVertexArray(my_vao);
  glBindBuffer(my_vbo);
  GLfloat verts[] = {0., 0.,  .5, .5,  0., .5 };
  glBufferData(GL_ARRAY_BUFFER, sizeof(verts), verts, GL_STATIC_DRAW);
  glEnableVertexAttribArray(position_loc);
  glVertexAttribPointer(position_loc, &hellip;);
glBindVertexArray(0);</p>

<p>// at render time
glBindVertexArray(my_vao);
  glDrawElements(&hellip;);
glBindVertexArray(0);</p>

<p>```
I won&rsquo;t be illustrating those here because I am focusing on OpenGL ES 2.0 for this article.</p>

<h5>Sending per-fragment varyings to the fragment shader</h5>

<p><em>Varyings</em> are per-fragment data the fragment shader uses to calculate the output colour of a fragment. Examples of this might be the normal of the surface at that point, or the material colour interpolated from the material colours of the surrounding vertices. A 3D program will often use this information along with the location of the sun (passed through a uniform) in its lighting calculations &ndash; a fragment on a surface directly facing a light source will have a brighter colour than one not.</p>

<p>The calculation of what value is passed into a varying is a little bit tricky. Fragment shaders get their per-fragment input indirectly from the vertex shaders through variables called <em>varyings</em>. Of course for any three vertices forming a triangle you could have hundreds of fragments, so the GPU takes the varyings coming out of the vertex shader for each vertex influencing a fragment and interpolates them before passing them into the fragment shader.</p>

<h4>The Vertex Shader</h4>

<p>Below is a simple vertex shader that expects two input variables from the application: the uniform <code>time</code> and the vertex attribute <code>vert_position</code>.</p>

<p>``` c++ a simple vertex shader
// per-primitive variables passed in from your application
uniform float time;</p>

<p>// per-vertex variables passed in from your application
attribute vec4 vert_position;</p>

<p>// output data to send to the fragment shader for each fragment derived from this vertex
varying vec4 frag_colour;</p>

<p>void main() {
  // mandatory! calculate the NDC coordinates of this vertex
  gl_Position = vec4(0.01 * sin(time), 0., 0., 0.) + vert_position;
  // frags go from black on the left side to red on the right side of the viewport
  frag_colour = vec4((vert_position.x + 1.) / 2., 0., 0., 1.);
}
<code>``
_Note:_ GLSL (the shader language) allows the specification of _precision_ for floating-point values, including all</code>vec<code>and</code>mat<code>types. Some versions of GLSL (at least GLSL ES) _require_ variable precisions to be specified in all declarations and parameter lists (I have omitted these for brevity in my examples.) Valid precision values are</code>lowp<code>,</code>mediump<code>and</code>highp<code>. In general</code>mediump<code>is what you want, although for colours you can use</code>lowp` without any problems.</p>

<h4>The Fragment Shader</h4>

<p>The fragment shader captures all the logic required to determine the colour of a fragment. This might be as simple as just returning a uniform RGBA value or might involve complex 3D shading calculations incorporating a number of light sources and shadow maps. I will explore the 3D stuff in a later article.</p>

<p>A simple fragment shader that works with the above vertex shader might look like this:
``` c++ a simple fragment shader
// per-primitive variables passed in from your application
uniform float time;</p>

<p>// interpolated from output data of vertex shader
varying vec4 frag_colour;</p>

<p>void main() {
  gl_FragColor = frag_colour;
}
```</p>

<p>Note that the <code>varying</code> variables are not passed directly from the vertex shader but is actually interpolated from the results of all vertex shader invocations for the vertices surrounding this fragment. This means that, for example, colour gradients look smooth between vertices.</p>

<h4>How the GPU writes fragments to the FrameBuffer</h4>

<p>In this context the <em>framebuffer</em> is the rendering target, which is usually either the viewport or a texture. (Framebuffers are also used for other purposes such as combining multiple rendering passes that I will go into in a later article.) When the GPU has collected all the fragments it is going to render, it goes through a series of per-fragment operations to determine what gets written to the framebuffer.</p>

<p>First the GPU checks to ensure this bit of the viewport is actually owned by this framebuffer, because is possible to have one viewport obscuring another. Next the fragment is tested against the scissor test region, the stencil buffer, then the depth buffer, if those capabilities are enabled in current context. If a fragment fails one of these tests it is discarded. Then the GPU performs a blending operation if enabled on the context, blending the fragment against what is read from the framebuffer before the render operation began. The resultant fragment is finally written to the framebuffer. Furthermore, if the framebuffer has <em>multisampling</em> enabled (for anti-aliasing) it may merge multiple fragment colours (or <em>samples</em> as they are called now) into a single pixel.</p>

<h4>Basic shader program example</h4>

<p>The partial application code below shows a basic vertex shader, fragment shader being invoked to render a square in the middle of the window.</p>

<p>``` c++ rendering a &lsquo;triangle strip&rsquo; primitive using a buffer of vertex positions
  // just define our shader in-line
  const char vertex_src[] = &ldquo;\</p>

<pre><code>uniform float time;       // passed in for whole primitive \n
attribute vec2 position;  // passed in with vertex data \n
attribute vec3 colour;    // passed in with vertex data \n
varying vec4 frag_colour; // passed out to frag shader  \n
void main() {                                           \n
  // this is where we would transform to NDC, but  our coordinates are already NDC
  // so just pass the position through as-is with a little animation \n
  gl_Position = vec4(position * sin(time), 0., 1.);     \n
  frag_colour = vec4(colour, 1.);
</code></pre>

<p>  }&ldquo;;
  const char fragment_src[] = &rdquo;\</p>

<pre><code>varying vec4 frag_colour; // passed in from vert shader (and interpolated) \n
void main() {                              \n
  // fragment colour is dark gray          \n
  gl_FragColor = frag_colour;
</code></pre>

<p>  }&ldquo;;</p>

<p>  // create shaders
  auto vertexShader = glCreateShader(GL_VERTEX_SHADER);
  glShaderSource(vertexShader, 1, &amp;vertex_src, NULL);
  glCompileShader(vertexShader);
  auto fragmentShader = glCreateShader(GL_FRAGMENT_SHADER);
  glShaderSource(fragmentShader, 1, &amp;fragment_src, NULL);
  glCompileShader(fragmentShader);</p>

<p>  // create shader program using the shaders
  GLuint shaderProgram = glCreateProgram();
  glAttachShader(shaderProgram, vertexShader);
  glAttachShader(shaderProgram, fragmentShader);
  glLinkProgram(shaderProgram);    // link the program
  glUseProgram(shaderProgram);    // and select it for usage</p>

<p>  // these are the NDC coordinates of a square on the viewport
  static const float vertexArray[] = {</p>

<pre><code> 0.0, 0.5,   1., .5, .5,    // x, y,  r, g, b,
 -0.5, 0.0,  .5, 1., .5,    // x, y,  r, g, b,
 0.0, 0.5,   1., .5, .5,    // x, y,  r, g, b,
 0.0, -0.5,  .9, .9, .9,    // x, y,  r, g, b,
 0.5, 0.0,   .5, .5, 1.,    // x, y,  r, g, b,
</code></pre>

<p>  };</p>

<p>  // we need this to pass the &lsquo;time&rsquo; uniform to the shaders
  auto time_loc = glGetUniformLocation(shaderProgram, &ldquo;time&rdquo;);
  // we need this to pass the &lsquo;position&rsquo; and &lsquo;colour&rsquo; attributes in to the vertex shader
  auto position_loc = glGetAttribLocation(shaderProgram, &ldquo;position&rdquo;);
  auto colour_loc = glGetAttribLocation(shaderProgram, &ldquo;colour&rdquo;);
  glEnableVertexAttribArray(position_loc);
  glEnableVertexAttribArray(colour_loc);
  // glVertexAttribPointer allows you to specify vertices in many ways, so its pretty complicated
  glVertexAttribPointer(position_loc, 2, GL_FLOAT, false, sizeof(GLfloat) * 5, sizeof(GLfloat) * vertexArray);
  glVertexAttribPointer(position_loc, 3, GL_FLOAT, false, sizeof(GLfloat) * 5, sizeof(GLfloat) * (vertexArray + 2));
  glDisableVertexAttribArray(position_loc);
  glDisableVertexAttribArray(colour_loc);</p>

<p>  while (!quit) {</p>

<pre><code>// ... clear viewport etc 

glUniform1f(time_loc, glfwGetTime());

glEnableVertexAttribArray(position_loc);
glEnableVertexAttribArray(colour_loc);
glDrawArrays(GL_TRIANGLE_STRIP, 0, 5);
glDisableVertexAttribArray(position_loc);
glDisableVertexAttribArray(colour_loc);

// ... swap buffers etc
</code></pre>

<p>  }
```
This should look something like this:</p>

<center>
  <iframe src="http://seshbot.com/assets/2015-05-13-gl1.html" width="320" height="200" scrolling="no" style="border: 2px solid black; -moz-box-shadow: black 2px 2px 2px;"></iframe>
  <br/>
  <a style="clear:both;" href="http://seshbot.com/assets/2015-05-13-gl1.html" target="_blank">click here to open in a separate window</a>
</center>


<h3>Immediate mode and the fixed function pipeline</h3>

<p>I discuss this more when discussing the different OpenGL versions below, but OpenGL 1 was much simpler to use than later versions, though much less powerful. OpenGL 1 operated using a <em>fixed-function pipeline</em> using an <em>immediate mode</em> API, where the programmer not only describes high-level primitives' individual vertices but also describe the lighting model to use, define several lights and set up materials to use during rendering. Retained mode allows all of this functionality to be executed on a shader program, which is written by the developer but run on the GPU directly. This is much more performant but requires a lot of extra work on the part of the developer.</p>

<p>The term immediate mode means that the drawing operations are explicitly executed in your client application each frame, which is considered slower because it ties the client application logic too closely with the GPU rendering operations, so the GPU is not able to make as many optimisations as it would if the instructions were on the GPU itself (retained mode.)</p>

<h2>Linear algebra (magic!)</h2>

<p>Linear algebra is the language of graphics programming. You <em>need</em> to learn some basics if you&rsquo;re going to tackle this stuff. I won&rsquo;t go into what a vector or matrix is here but you will have to learn the basics of their form and function if you don&rsquo;t already know them.</p>

<p>The most basic understanding you should have is that vectors are usually used to describe coordinates in space and directions, and matricies are used to describe transformations (translation, scale, rotation, shear etc) to those vectors. Another thing to note is that a single matrix may represent an accumulation of many different transformations performed in sequence, so if I said (in pseudo-code) <code>auto m = translate * scale * rotate</code>, then any time I multiply <code>m</code> by a vector it will have the same effect as performing all of those transformations at once &ndash; amazing!</p>

<p>Once again, the OpenGL API does not help you in dealing with matricies or vectors, but there is a great supporting library that does &ndash; <a href="http://glm.g-truc.net/">GLM</a>.</p>

<p>There are two ways the elements in a matrix may be stored &ndash; OpenGL programmers often use <em>column-major</em> matrix layouts. This is a convention only, but is generally used in the official documentation and in OpenGL support libraries such as GLM. The reason this is important is that unlike scalar multiplication, matrix multiplication is not <em>commutative</em>, meaning <code>A * B</code> does not equal <code>B * A</code>. The main impact of using column-major vs row-major matricies is the order of multiplications must be reversed to have the same effect. In column-major (the most usual) you would accumulate your transformations to the left, so if you want to first rotate (<em>R</em>) then scale (<em>S</em>) then translate (<em>T</em>) last, you would execute <code>T * S * R</code>. A more common example would be when calculating the <em>model view projection</em> matrix it would be accumulated as <code>mat4 mvp = P * V * M</code>. When calculating this with a <em>row-major</em> library you would be expected to accumulate it as <code>mat4 mvp = M * V * P</code>. Converting a matrix to or from column-major to row-major is known as <em>transposing</em> the matrix.</p>

<p>A common term in linear algebra is the <em>identity matrix</em>. This is a matrix <em>I</em> where multiplying it with another matrix <em>A</em> gives that matrix (<code>I * A = A</code>.) It is easily recognisable as it is entirely made of 0s with 1s on the diagonal.</p>

<p>Another generally useful matrix operation is the <em>inverse</em> of a matrix. The inverse of a matrix A is the matrix required to multiply with A so that the result is the identity matrix. In other words, <code>Ai * A = I</code>. This is useful when rolling back a matrix multiplication. If you have the <em>model view projection</em> matrix <code>mat4 mvp = proj * view * model</code>, you can find the <em>model view</em> matrix by calculating the inverse projection <em>inv_proj</em> and calculating: <code>mat4 mv = inv_proj * mvp</code>.</p>

<p>Vector operations are even more interesting. A few things I want to point out here are <em>dot product</em>, <em>cross product</em> and the difference between <em>positional coordinates</em> and <em>directional coordinates</em>.</p>

<p>The <strong>dot product</strong> operation (sometimes known as the <em>inner product</em>) is used for many purposes; the dot product of two vectors A and B is a scalar (not a vector) number that is the sum of the products of their components (e.g., <code>auto a_dot_b = A.x * B.x + A.y * B.y + A.z * B.z</code>). It turns out that this simple formula gives you the cosine of the angle between those vectors multiplied by their magnitudes (<code>|A||B|cos(Ɵ)</code>), which is really useful because:</p>

<ul>
<li>if the vectors are unit vectors (they each have magnitude of 1) the dot product will just give you <code>cos(Ɵ)</code> which is a number between 0 and 1, where 0 implies that they are perpendicular to each other (at 90 degrees) and 1 implies they are parallel. This is great for calculating how much light should bounce off a surface if the light direction is one vector and the surface normal is the other.</li>
<li>if the vectors are both unit vectors you can inverse cos the dot product to find the angle between the vectors (<code>auto angle = acos(dot(A, B))</code>)</li>
<li>you can find the projection of vector A onto vector B by finding the dot product of A and B then dividing the result by the length of A.</li>
<li>calculating the dot product of a vector with itself will give you the distance squared. If you are checking to see which vector is longer, you can just compare their squared distances (saving you a square root operation)</li>
</ul>


<p>The <strong>cross product</strong> is another simple formula that gives you a vector that is perpendicular to two given vectors. In other words, if you have vectors A and B that both lie along the same surface, calculating <code>cross(A, B)</code> will give a vector that represents the normal to that surface. This normal vector will also follow the <em>right-hand rule</em> as pictured below (Note: you will usually want to normalise your normal before using it, so lighting calculations can dot product them straight out of the box!)</p>

<p><img class="center <a" src="href="http://upload.wikimedia.org/wikipedia/commons/d/d2/Right_hand_rule_cross_product.svg">http://upload.wikimedia.org/wikipedia/commons/d/d2/Right_hand_rule_cross_product.svg</a>" width="200" height="200" title="&ldquo;Cross product follows the right-hand rule&rdquo;" ></p>

<h2>OpenGL versions and extensions</h2>

<p>A constant frustration when reading documentation and code examples is that OpenGL 1.0 is <em>worlds apart</em> from OpenGL 2.0. Many of us would have been well served if they had given it a totally different name, so different are the products!</p>

<p><strong>OpenGL 1</strong> functions in what is now called <em>immediate mode</em> and is considered deprecated. In this version the programmer describes the scene lights, materials and fog, then notifies the driver of each polygon explicitly. The driver would then render the scene using its own internal lighting model using the described lights and materials. This is called the <em>fixed function pipeline</em>.</p>

<p>``` c++ OpenGL 1 example code snippet
// configure light
glEnable(GL_LIGHTING);     // deprecated
glEnable(GL_LIGHT0);       // deprecated
glLightfv(GL_LIGHT0, GL_AMBIENT, {.4f, .1f, .1f}); // deprecated</p>

<p>// draw a primitive
glPushMatrix();            // deprecated
// matrix operations will apply to all vertices until glPopMatrix()
glLoadIdentity();          // deprecated
glRotatef(3.14f, 0., 0., 1.); // deprecated
glBegin(GL_TRIANGLE);      // deprecated
  glVertex3f(.0, .0., .0); // deprecated
  glVertex3f(.1, .0., .0); // deprecated
  glVertex3f(.1, .1., .0); // deprecated
glEnd();                   // deprecated
glPopMatrix();             // deprecated
glFlush();
```</p>

<p><strong>OpenGL 2</strong> introduced shaders, yet still provided compatability with the above described model. Shaders had access to the state declared in the fixed function pipeline through standard global variables (such as the <code>gl_LightSource[]</code> array and the <code>gl_FrontMaterial</code> variable sent from the client and the <code>gl_ModelViewProjection</code> matrix which was generated by the driver.) The vertex shader can invoke the standard fixed-function pipeline functionality by callling <code>gl_Position = ftransform();</code>.</p>

<p><strong>OpenGL 3</strong> completely deprecated the immediate mode fixed-function pipeline. Of course this introduced backwards-compatability issues so they also introduced several <em>profiles</em> to allow backward compatability to be optionally compiled in. The <em>compatability profile</em> can be requested to enable deprecated features, while the <em>core profile</em> disallows the use of these features.</p>

<p>Since OpenGL 2 however there have been no truly large architectural changes (unfortunately.) Changes have focused around adding features (new types of shaders, for example) to provide better performance and more generally useful aspects of existing functionality. This is a shame because there are still a lot of problems with OpenGL that many developers want addressed &ndash; the original goal of OpenGL 3 was to include massive refactorings to remove all the global state (more on this later) but vendors objected so strenuously that this work was put off until <em>GLNext</em> which is now known as <a href="https://www.khronos.org/vulkan">Vulkan</a>.</p>

<p>OpenGL shaders are written in their own language &ndash; <strong>GLSL</strong>. GLSL has its own varied syntax between versions, and to make things even more complicated they support the notion of extensions. The best bet is to decide on which version of OpenGL you will be learning and learn the GLSL appropriate to that version. They are all quite similar in form but are syntactically incompatible with each other.</p>

<p>Some mention should be made regarding extensions.  Extensions are often touted as a great feature in OpenGL not available in other graphics libraries such as DirectX. New commands and enumerations are often added to the OpenGL API by vendors as extensions, and then if this functionality proves popular it becomes formalised as part of the API in a later version.</p>

<p>Each OpenGL version has a known set of extensions. These can be browsed in the Khronos reference implementations &ndash; for example the OpenGL ES 2 extensions are in the <a href="https://www.khronos.org/registry/gles/api/GLES2/gl2ext.h">GLES2/gl2ext.h</a> header file. There are idiomatic ways to detect support for and load these extensions using system calls but most people use an <strong>extension loader</strong> of some type. A very popular extension loader is the <a href="http://glew.sourceforge.net/">OpenGL Extension Wrangler Library</a> (also known as GLEW.)</p>

<p>If you want some functionality not natively available in your chosen version of the OpenGL API you will often find an extension that provides that functionality. The problem is, extensions are not supported uniformly on all platforms with all drivers, so quite often you&rsquo;ll have to work around the missing functionality in some platform anyway. This severely limits the usefulness of extensions, and in general you should try to do without them if possible.</p>

<p>I personally do use a few extensions in a few circumstances: either to get around eccentricities of a particular platform (e.g., some Windows platforms use BGRA instead of RGBA framebuffer formats, which are only available through the GL_EXT_texture_format_BGRA8888 extension) or if I have through experimentation determined that an extension is very broadly supported.</p>

<p>Here&rsquo;s a quick summary of what I understand of the different OpenGL versions:</p>

<h3>OpenGL 1 &ndash; high level and slow but simple</h3>

<ul>
<li>Immediate mode only (fixed function pipeline)</li>
<li>no shaders</li>
<li>a lot of people still use this when demonstrating functionality</li>
<li>only version natively supported by Windows</li>
</ul>


<h3>OpenGL 2 &ndash; shaders run on the GPU</h3>

<ul>
<li>first shader-based version</li>
<li>vertex and fragment shaders</li>
<li>still have access to the fixed function pipeline &ndash; even within shaders</li>
</ul>


<h3>OpenGL 3 &ndash; a controversial release</h3>

<ul>
<li>framebuffers for rendering to non-screen targets (e.g., render to a texture)</li>
<li>vertex array objects (VAOs) allow great performance boosts by quickly binding and unbinding whole groups of buffers and attribute bindings</li>
<li>geometry shaders (modify/extend geometry of primitives)</li>
<li>significant (breaking!) changes to GLSL shader language</li>
<li>deprecated most 1.0 functionality (immediate mode stuff) introducing compatability and core modes

<ul>
<li>compatability mode gives access to the old fixed function pipeline</li>
<li>core mode does not</li>
</ul>
</li>
</ul>


<h3>OpenGL 4 &ndash; modernisation, performance and professional enhancements</h3>

<ul>
<li>OpenGL 4.0 goal was to achieve parity with Direct3D 11</li>
<li><a href="http://arstechnica.com/information-technology/2014/08/opengl-4-5-released-with-one-of-direct3ds-best-features/">OpenGL 4.5</a> goal was to achieve parity with Direct3D 12</li>
<li>tesselation shaders introduce extra polygons for &lsquo;denser&rsquo; meshes with smoother curves</li>
<li>compute shaders for using the GPU for non-graphics computations (GPGPU stuff)</li>
<li>GPGPU compute shader uses SPIR &ndash; an intermediate language based on LLVM</li>
<li>Direct State Access &ndash; mitigate long-standing architectural problems with immediate mode</li>
<li>modern OSX supports up to OpenGL 4.1</li>
</ul>


<h3>OpenGL ES &ndash; simplified for embedded systems</h3>

<ul>
<li>OpenGL ES 1.0 based on OpenGL 1.3</li>
<li>OpenGL ES 1.1 based on OpenGL 1.5</li>
<li>OpenGL ES 2.0 based on OpenGL 2.0

<ul>
<li>WebGL is based on OpenGL ES 2.0</li>
<li><a href="https://code.google.com/p/angleproject/">Google ANGLE project</a> provides OpenGL ES 2.0 support on Windows (wraps DirectX API)</li>
</ul>
</li>
<li>OpenGL ES 3.0 full subset of OpenGL 4.3

<ul>
<li>GLSL ES 3.0 based on GLSL 3.3</li>
<li>similar to OpenGL 3 but without geometry shaders</li>
<li>supported by modern iOS and Android devices</li>
<li><a href="https://code.google.com/p/angleproject/wiki/Update20150105">experimental support</a> in Google ANGLE project</li>
</ul>
</li>
</ul>


<h3>WebGL</h3>

<ul>
<li>based on OpenGL ES 2.0</li>
<li>I don&rsquo;t know much about this yet, but <a href="http://webglfundamentals.com">WebGL Fundamentals</a> is a great resource</li>
</ul>


<h3>Vulkan &ndash; GLNext, lots of hype but not much information yet</h3>

<ul>
<li>get away from immediate mode single-threaded global state context heritage</li>
<li>allow shaders to be written in a variety of languages</li>
<li><a href="http://arstechnica.com/gadgets/2015/03/khronos-unveils-vulkan-opengl-built-for-modern-systems/">http://arstechnica.com/gadgets/2015/03/khronos-unveils-vulkan-opengl-built-for-modern-systems/</a></li>
</ul>


<p>I have chosen to do most of my experimentation in OpenGL ES 2. This should give me the broadest platform availability as well as being compatible with WebGL for web demonstrations. I have resorted to using a few extensions that are supported on the platforms I use where necessary (e.g., to get anti aliasing), though I try to avoid this where possible.</p>

<h2>Challenges you will face</h2>

<ul>
<li>Having to learn support libraries in addition to the OpenGL API</li>
<li>Cross platform support is difficult as many features are not uniformly supported</li>
<li>Debugging GL state in different platforms &ndash; OpenGL debugging tools are not great, and there are none that work cross-platform</li>
<li>Multithreading correctly is extremely difficult &ndash; if possible just do all your rendering on one thread!</li>
<li>Managing extensions can be a pain in the ass</li>
<li>State management &ndash; OpenGL uses global state, so it is impossible to write optimal abstractions because cannot encapsulate state, and state querying is prohibitively expensive. So you end up redundantly setting state that has often already been set.</li>
<li>lots of problems: <a href="http://richg42.blogspot.jp/2014/05/things-that-drive-me-nuts-about-opengl.html">http://richg42.blogspot.jp/2014/05/things-that-drive-me-nuts-about-opengl.html</a></li>
</ul>


<h3>Documentation and tutorials</h3>

<p>As I mentioned there&rsquo;s not a lot of great documentation out there. I started to create <a href="http://cechner.github.io/">my own responsive OpenGL documentation</a> and then discovered that there&rsquo;s already a pretty sweet one out there called <a href="http://docs.gl/">docs.gl</a>. Docs.gl is great because it makes it clear which commands are supported in which versions of OpenGL &ndash; something that&rsquo;s hard to figure out from the more official sources.</p>

<p>A great beginner tutorial is <a href="http://open.gl/">open.gl</a> &ndash; it&rsquo;s modern, minimalistic and well written. A very similar-seeming tutorial series that goes into more advanced techniques is <a href="http://learnopengl.com/">Learn OpenGL</a>. There used to be a fantastic series called the ArcSynthesis tutorials but they seem to have died. There is a <a href="http://www.pdfiles.com/pdf/files/English/Designing_&amp;_Graphics/Learning_Modern_3D_Graphics_Programming.pdf">PDF version of their content</a> that seems pretty good though.</p>

<p><a href="http://ogldev.atspace.co.uk/index.html">OGLdev</a> is a series of tutorials that go into more depth than the open.gl site above, and is in bite-sized chunks for easier consumption.</p>

<p>If you want to step through some sample code, download the <a href="https://github.com/progschj/OpenGL-Examples">OpenGL-Examples</a> github repository. It seems pretty easy to use and goes into fairly advanced topics.</p>

<p>If you want <em>really really detailed</em> runthrough of the graphics pipeline, have a look at <a href="https://fgiesen.wordpress.com/category/graphics-pipeline/">the ryg blog</a>. I haven&rsquo;t made my way through it yet but I really want to. Fabien Gleson seems to be pretty knowledgable about not only low level rendering details but also general low level computing concepts in general.</p>

<p>Finally, if you&rsquo;re into WebGL you should check out Gregg Tavares' <a href="http://webglfundamentals.org/">WebGL Fundamentals</a>. Gregg has a lot of experience working on WebGL in Chrome and game programming in general and has made some fantastic javascript support libraries that make every-day WebGL much simpler.</p>

<p>As I mentioned before though, you can easily try reading the specs yourself. Google is your friend here &ndash; search for the specific version + &lsquo;spec&rsquo; and it will <a href="https://www.google.com.au/search?q=opengl+3+spec">usually be the first hit</a>.</p>

<h2>Debugging</h2>

<p>There are <a href="https://www.opengl.org/wiki/Debugging_Tools">many different OpenGL debugging tools</a> on different platforms and they are all generally pretty bad. I haven&rsquo;t spent a lot of time with any of them so please tell me if you find a good one!</p>

<p>If you&rsquo;re on OSX you can give the <a href="https://developer.apple.com/library/mac/documentation/GraphicsImaging/Conceptual/OpenGLProfilerUserGuide/Introduction/Introduction.html">OpenGL Profiler</a> a go &ndash; it allows you to set breakpoints on certain GL calls, look at frame buffers (though I found this difficult to do) and much more.</p>

<p>Windows users should check out <a href="https://github.com/baldurk/renderdoc">RenderDoc</a>, which allows you to track API calls, view render buffers and a lot of other stuff, for both OpenGL and DirectX. You can also invoke the DLL directly to dump various information from within your application. I haven&rsquo;t used it myself though so I won&rsquo;t go on further.</p>

<p>It&rsquo;s also a great idea to have some macros that optionally call <code>glGetError()</code> after every OpenGL call you make so you can easily track down where things begin to go awry. Unfortunately though querying the context can be fairly expensive so you should make it easy to disable this macro when things are not going awry.</p>

<p>Feel free to copy-paste this into your codebase:
``` c++
// #define DEBUG_OPENGL_COMMANDS // uncomment this to enable error checking</p>

<h1>ifdef DEBUG_OPENGL_COMMANDS</h1>

<p>  void checkOpenGLError(const char<em> function, const char</em> file, int line) {</p>

<pre><code>auto err = glGetError(); if (err == GL_NO_ERROR) return;
const char * err_msg = "unrecognised";
switch (err) {
  case GL_INVALID_ENUM: err_msg = "GL_INVALID_ENUM"; break;
  case GL_INVALID_VALUE: err_msg = "GL_INVALID_VALUE"; break;
  case GL_INVALID_OPERATION: err_msg = "GL_INVALID_OPERATION"; break;
  case GL_STACK_OVERFLOW: err_msg = "GL_STACK_OVERFLOW"; break;
  case GL_STACK_UNDERFLOW: err_msg = "GL_STACK_UNDERFLOW"; break;
  case GL_OUT_OF_MEMORY: err_msg = "GL_OUT_OF_MEMORY"; break;
  case GL_INVALID_FRAMEBUFFER_OPERATION: err_msg = "GL_INVALID_FRAMEBUFFER_OPERATION"; break;
  default:
}
// on Windows call ::OutputDebugString 
fprintf(stderr, "OpenGL error '%s' (0x%04x) called from %s in file %s line %d\n", 
  err_msg, err, function, file, line);
</code></pre>

<p>  }</p>

<h1>define GL_VERIFY(stmt) do { stmt; checkOpenGLError(#stmt, <strong>FUNCTION</strong>, <strong>FILE</strong>, <strong>LINE</strong>); } while (0)</h1>

<h1>define GL_CHECK() do { checkOpenGLError(<strong>FUNCTION</strong>, <strong>FILE</strong>, <strong>LINE</strong>); } while (0)</h1>

<h1>define GL_IGNORE(stmt) do { GL_CHECK(); stmt; glGetError(); } while (0)</h1>

<h1>else</h1>

<h1>define GL_VERIFY(stmt) stmt</h1>

<h1>define GL_CHECK()</h1>

<h1>define GL_IGNORE(stmt) stmt</h1>

<h1>endif</h1>

<p>```</p>

<p>Then you just wrap all your function calls like so: <code>GL_VERIFY(glDrawElements(...))</code>. If you want to just check for errors at a particular point in your code, just call <code>GL_CHECK()</code>.</p>

<p>One final interesting note about debugging OpenGL: If you are using a Google&rsquo;s ANGLE OpenGL driver that you compiled yourself you can step into it, so if you start getting vague sounding errors like <code>GL_INVALID_FRAMEBUFFER_OPERATION</code> but want to know specifically what the problem is, you can step into the ANGLE DLLs yourself to see which part of their validation fails. It&rsquo;s like running a own fully-compliant validation layer in your own client code.</p>

<h2>Upcoming</h2>

<p>Now that I&rsquo;ve gotten all the basic stuff out of the way I&rsquo;d like to go into a bunch of other more advanced things that I thought wasn&rsquo;t particularly easy to get help with. In no particular order:</p>

<ul>
<li>Pros and cons of writing an OpenGL wrapper library (<a href="https://github.com/seshbot/glpp">my glpp library</a>)</li>
<li>Building and using Google&rsquo;s ANGLE library</li>
<li>Using OpenGL for 2D:

<ul>
<li>setting up the GL context (depth buffer)</li>
<li>basic 2D coords</li>
<li>drawing primitives</li>
<li>simple texture</li>
<li>using orthographic projection with 2D</li>
</ul>
</li>
<li>Using OpenGL for 3D:

<ul>
<li>setting up the GL context (blending, face culling)</li>
<li>3d coordinate system (plus MVP, normal matrix)</li>
<li>drawing primitives</li>
<li>perspective projection and the frustum</li>
</ul>
</li>
<li>3D lighting</li>
<li>3D shadows</li>
<li>Particle systems</li>
<li>Texturing (textures, texture unit and samplers), sampling, blending, alpha discard, stencil testing, <a href="http://stackoverflow.com/questions/9224300/what-does-gltexstorage-do">glTexStorage</a></li>
<li>Multi pass rendering (using FrameBuffers):

<ul>
<li>Render scene &ndash;> FBO &ndash;> texture colour buffer &ndash;> screen rectangle &ndash;> post effect frag shader &ndash;> screen</li>
<li>Post processing (HSV and gamma correction)</li>
</ul>
</li>
<li>Loading models and animations (using assimp)</li>
<li>Rendering text (using stb)</li>
<li>Using Qt/QML with OpenGL</li>
<li>Object Picking in a 3D scene</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The best way for programmers to interview programmers]]></title>
    <link href="http://seshbot.com/blog/2014/01/23/the-best-way-for-programmers-to-interview-programmers/"/>
    <updated>2014-01-23T20:56:52+00:00</updated>
    <id>http://seshbot.com/blog/2014/01/23/the-best-way-for-programmers-to-interview-programmers</id>
    <content type="html"><![CDATA[<p><strong>Every programmer should know how to give technical interviews.</strong> Although you will rarely be judged on your interviewing abilities, it is definitely in your best interest to help ensure your company is hiring effective programmers. If you manage to surround yourself with great programmers you&rsquo;ll foster a much better environment in which you can become a great programmer yourself. If you find yourself surrounded by programmers you consider sub-par you&rsquo;ll end up supporting their crappy software.</p>

<p>Surround your pullquote like this {" text to be quoted "}</p>

<p>I believe this advice applies to all programmers &ndash; of course interviewing is rarely in your job description, but it is definitely in your best interest to make sure interviews get done well at your company. Unfortunately most companies will allow anyone to interview and won&rsquo;t have a good way to tell that they&rsquo;re not doing it well, ultimately damaging the work environment.</p>

<!-- more -->


<h2>The mediocre way</h2>

<p>Typically interviewers will be given a list of questions to ask and some criteria by which to judge the answers to these questions. Perhaps the interview will follow some format of scripted grilling, perhaps a practical section, and some kind of open two-way forum. Following a script is supposed to allow the candidates to be compared fairly on a level playing field. The practical section might consist of taking apart a printed code sample or even writing some code, and the forum provides the candidate the opportunity to explain their situation and determine if they actually want to work at the company themselves.</p>

<p>This has a lot of problems however:</p>

<ul>
<li>if the interviewer completely controls the questions the candidate may come across poorly because they don&rsquo;t happen to have experience in the specific fields chosen, or because he or she is just very nervous</li>
<li>it promotes a one-size-fits-all mentality, which will lead to a homogenous work environment</li>
<li>it is too easy for the interviewer to just phone it in without truly probing for deeper knowledge. In fact this becomes more likely as the interviewer becomes bored of the questions</li>
<li>it is too easy for candidates to blag &ndash; some people can sound like experts when all they have is brash confidence and a few keywords. Plus, recruitment companies often coach candidates on what questions might be asked so those keywords are likely to be there</li>
<li>quiz-type interviews often fail to test higher-level proficiencies, such as communicating architecture and design information</li>
</ul>


<p>I think it&rsquo;s well worth throwing away the &lsquo;standardised&rsquo; approach to testing technical capabilities. There is too much risk of false negatives <em>and</em> false positives along this approach &ndash; a poor programmer may have been well coached, and a good programmer might have an unfortunate gap right where your questions probe (surely I could have worded that better&hellip;)</p>

<h2>The Seshbot way</h2>

<p>While it is important to cover several bases in an interview, the most important question I always ask is <strong>&lsquo;what was the most recent interesting project in which you were heavily involved?&rsquo;</strong> I almost always spend most of the interview going deep into a few questions that branch off the answers to this question.</p>

<p>Surround your pullquote like this {" text to be quoted "}</p>

<p>Once the candidate has explained the project, the interviewer can then challenge the details of that project as a way of testing the candidate&rsquo;s knowledge of that system and the reasoning behind its design. The types of question would vary depending on the workplace, but at my last job for example I probably would have asked:</p>

<ul>
<li><em>architecture:</em> please outline for me (ideally on a whiteboard or paper) how the whole system hangs together. I might expect the candidate to talk about the systems involved, data flows, security considerations, or data storage and integrity for example.</li>
<li><em>design quality attributes and compromises:</em> how does this system provide, or how would you adjust this architecture to provide, various quality attributes (high availability, reliability, scalability, performance, security)? What compromises would these changes entail?</li>
<li><em>design:</em> if I were a new developer working on this team, how would you describe the design of this system to me?</li>
<li><em>specifics:</em> I also spend a lot of time asking &lsquo;what is the purpose of that service/component/class&rsquo;</li>
</ul>


<p><span style="color: green; font-weight: bold;">Pros:</span> This addresses all of the problems I outlined with the more traditional approach:</p>

<ul>
<li>the candidate is talking about something they are presumably proud of and know inside-out. If they can&rsquo;t talk happily on this subject they won&rsquo;t be able to talk about any subject</li>
<li>the candidate is more able to show you what their particular skill set entails, and what they believe is their forte without them awkwardly spelling it out (through stupid &lsquo;what are your strengths and weaknesses&rsquo; questions)</li>
<li>the interviewer must be engaged in the process in order to ask relevant questions. It is in fact a great way to keep the interviewing process more interesting &ndash; and even educational &ndash; for the interviewer</li>
<li>because it is a &lsquo;deep&rsquo; and detailed question, it is very difficult to blag. Someone might be able to re-draw arcitectural diagrams they had to become familiar with in the past, but they would still need to know <em>why</em> the system was configured that way to answer the questions</li>
<li>the test focuses on communication of ideas, which I believe is the most important skill for a team member to have. You may or may not agree with this, but you certainly want to have tested it</li>
</ul>


<p><span style="color: red; font-weight: bold;">Cons:</span> There are some caveats however:</p>

<ul>
<li>do not let the candidate run the interview. It can be too easy to tackle a question by diverting into another topic &ndash; make sure that the candidate answers the questions you ask to your satisfaction</li>
<li>keep an eye on the time, because although you want to go deep on few questions, there&rsquo;s no point going deeper into a topic once the candidate has demonstrated their knowledge in that area</li>
<li>it can be difficult to recall what you discussed later when you&rsquo;re trying to make a decision about the candidate. Take notes constantly, and summarise the interview in an email directly after you finish it</li>
<li>if the candidate chooses to talk about technologies with which you&rsquo;re not very familiar yourself you&rsquo;ll be forced to ask largely high-level questions. This is not terrible as I believe you can still figure out if they know their stuff, but you might end up having to take their word on a few things</li>
</ul>


<h2>How to measure success</h2>

<p>Other than retention rates I am not certain of how to measure if your technique is successful, but in my experience most prospective programmers in the job market are not very good (something like 1-in-10), and so if you&rsquo;re hiring a high percentage of applicants you&rsquo;re probably letting in a few duds.</p>

<p>This might seem depressing and I feel bad about saying that, but this is my blog and I want to put my honest opinion up here. Take it as you will. <a href="http://www.codinghorror.com/blog/2007/02/why-cant-programmers-program.html">Others agree with me</a> however.</p>

<p>Because of this it is a very good idea to have a basic pre-screening done before the proper interview. It should be very simple and not take too long &ndash; a simple test should be sufficient to weed out most of the poor programmers, and you need to remain respectful of the candidate&rsquo;s time as they are interviewing <em>you</em> as well.</p>

<h2>What about live coding?</h2>

<p>Here&rsquo;s an anecdote: once while applying for a job at a large investment bank I was given an almost purely practical interview. On arriving I sat down with the lead developer and chatted for a bit before heading into the programming room to do some pair programming. Sweet right?</p>

<p>Anyway I sat at this Windows box with Visual Studio on it (this was for a largely .Net job) and was given the basic <a href="http://c2.com/cgi/wiki?FizzBuzzTest">fizzbuzz test</a>. There is a lot of controversy around this interview technique but I actually enjoy practical questions so didn&rsquo;t have a problem with the idea. The interview was a shambles however, and although I did well I count it as a terrible interview that did not serve its purpose at all.</p>

<p>First I had trouble with their development environment. The machines at this bank were ridiculously slow &ndash; they were running a special (presumably super-secure) build of Windows 95 that was by now literally something like 13 years old, and they couldn&rsquo;t change because it would be too costly to get a special build of a more modern Windows. So we sat there for a long time waiting to get all the software up and running, and every button click seemed to require the human genome to be re-calculated.</p>

<p>The next problem was that making sure that the interviewer and I were on the same page took way too long. I did the fizz-buzz program, and decided to show him how I would do it declaratively (with LINQ) so we could talk about the benefits of declarative programming. Of course if I were interviewing someone I would want them to solve the problem as simply as possible first, then make it more complicated later. I know through experience that many candidates think that they are expected to say things like <em>I would never start programming without a firm set of requirements!</em> or some other approach showing that they know about all areas of software development.</p>

<p>Turns out this guy wanted me to demonstrate some test-driven development, and so was expecting me to have started by writing tests. So OK, I figured I&rsquo;d download <a href="http://www.specflow.org/">SpecFlow</a> because I had used it successfully recently and really liked it. But no, this is a bank I&rsquo;m sitting in so we can&rsquo;t just download stuff &ndash; things have to go through a days-long process of scanning and justification before new software is installed. So I ask what they use and he mentioned some package or other. So I had to read the docs on that, and we had trouble actually running it on this machine in the end because we had to configure some directories in Visual Studio or something, and much more time was wasted.</p>

<p>I&rsquo;m not saying it can&rsquo;t work, but in my opinion live coding is fraught with dangers. A much better alternative is to browse the applicant&rsquo;s source code online for projects they have worked on. Realistically though the number of people I have interviewed that had  online code I could evaluate was exactly 1 (I did hire him though!)</p>

<h2>General interviewing advice</h2>

<p>Obviously interviewing is a difficult thing to do well. You have a small amount of time to decide whether to commit your company to filling a permanent seat on your team, which is a <em>huge</em> deal. Probably practice is the only thing that helps mitigate this (and the Seshbot technique of course) but here&rsquo;s a few other pointers that come to mind:</p>

<ul>
<li><em>take notes constantly!</em><br />One of the problems you can encounter with a less scripted interview is that when summarising how the interview went, it can be difficult to recount all the things you talked about. I tend to fill a couple of pages in my notebook with bullet-pointed scribbles during each interview</li>
<li><em>know when you know</em><br />The best interview I ever gave was a really nice guy called Phil. Only a few minutes into the interview I realised that he had probably handled my questions better than I could have. So I just sat back and told him &lsquo;I don&rsquo;t think we really need to go any further&rsquo;, then I walked out and asked HR to give him an offer on the spot. I was always happy that I did that.</li>
<li><em>do basic screening</em><br />If no screening has been performed it is worthwhile asking a couple of basic language-specific technical questions to make sure you&rsquo;re not wasting your time</li>
<li><em>the interviewer doesn&rsquo;t need to know the answer to all the questions he or she asks</em><br />You can choose to pretend that you really know and are just testing their knowledge, but if I don&rsquo;t know something I usually don&rsquo;t mind asking questions in an outright manner &ndash; there&rsquo;s no need to maintain a facade of superiority and being honest helps promote a candid atmosphere</li>
<li><em>have a standard approach to summarising the interview</em><br />At the end of the day you need to provide some quantifiable metric by which to decide who to hire. Plus HR and the recruiter usually require some specific information. At my last place HR was happy with just the name, date, my decision/recommendation, and justification for that decision</li>
<li><em>if you&rsquo;re doing a phone interview (unfortunate!) follow a script for the scaffolding for the interview</em><br />This keeps you from spending too much time on the plesantries and keeps you from waffling on</li>
<li><em>I usually like to ask if there&rsquo;s any good books a person would recommend, and ask what they got from it</em><br />It may be curmudgeonly of me but I don&rsquo;t think a person can learn fundamentals very well from Stack Overflow alone</li>
<li><em>Work with your recruiter, but assume they are coaching the candidates</em><br />A cool side-effect of this techinique is that it doesn&rsquo;t matter if candidates are coached &ndash; I told our recruitment agencies to feel free sharing information about our interviews with candidates</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Client-side authentication with ember and rails]]></title>
    <link href="http://seshbot.com/blog/2014/01/22/client-side-authentication-with-ember-and-rails/"/>
    <updated>2014-01-22T00:30:27+00:00</updated>
    <id>http://seshbot.com/blog/2014/01/22/client-side-authentication-with-ember-and-rails</id>
    <content type="html"><![CDATA[<p><em>This post follows on from the previous post <a href="/blog/2014/01/15/creating-a-rails-plus-ember-app-from-scratch/">creating and deploying a rails + ember app from scratch</a>. In that post I created a basic ember application being served from rails. Here I&rsquo;ll be adding user authentication to the application I created using the &lsquo;edge template&rsquo; option I discussed, which uses the &lsquo;ember-rails&rsquo; gem to establish the basic connectivity.</em></p>

<p>I do not want to implement authentication and authorization myself. It is tricky to get right and tends to cause huge damage when it goes wrong in production.</p>

<p>So I have spent at least three full days looking at various solutions I can build into my simple Ember/Rails application and spent a lot of time experimenting.</p>

<p>This post describes my current understanding of authentication for web applications, and the approach I used to implement a basic authentication system I put up on heroku at <a href="http://seshbot.herokuapp.com">http://seshbot.herokuapp.com</a></p>

<p>If you want to see the source code, have a look at <a href="https://github.com/seshbot/new-ember-rails-app">https://github.com/seshbot/new-ember-rails-app</a></p>

<p><img class="center" src="/images/upload/2014-01-22-seshbot-login.png" title="&ldquo;Oh jeez I already forgot my password&rdquo;" ></p>

<p><em>NOTE: this is very text-heavy because after three full days I decided not to spend too long on this blog post. Therefore there are no images at this time. I may update it later to have some nice UML or screenshots, but that time is not now.</em></p>

<!-- more -->


<h2>Learning the basic concepts</h2>

<p>I found a few very detailed introductions to client-side authentication with ember which helped me through all stages of implementation of my system. I highly recommending going through the following resources and comparing the different approaches' overlaps and differences. I also got a lot of value out of re-visiting them after I finished implementing my own solution, because it made me think about some of the trade offs I had made.</p>

<h3>Watch these awesome client-side suthentication videos</h3>

<p><a href="http://www.embercasts.com/">http://www.embercasts.com/</a> covers the client-side concepts of authentication with Ember in <a href="http://www.embercasts.com/episodes/client-side-authentication-part-1">part 1</a> and <a href="http://www.embercasts.com/episodes/client-side-authentication-part-2">part 2</a> of their &lsquo;Client Side Authentication&rsquo; videos. Specifically:</p>

<ul>
<li>client token authentication concepts</li>
<li>sending auth request to the server and saving the token (in a &lsquo;login&rsquo; controller)</li>
<li>setting up controllers</li>
<li>catching &lsquo;unauthorised&rsquo; error responses and redirecting to login pages</li>
<li>keeping a sane workflow so the login transitions the user back to their original page</li>
<li>storing the auth token in local storage so page refresh doesn&rsquo;t reset it <em>(note: I used cookies instead of local storage)</em></li>
<li>preventing the unauthorized server request if client knows it doesn&rsquo;t yet have a token</li>
</ul>


<p>This doesn&rsquo;t cover the server side, or anything to do with Rails or any other authenticating server specifically &ndash; he used a home-grown demonstration Node.js server for the demonstration. Also doesn&rsquo;t specifically cover authorization (I can see users but can&rsquo;t see their emails for example.) The ember front-end polish in there is all nice though, and I found it very helpful to revisit these videos after I had a basic system in place, in order to add nice error messages and improving the &lsquo;view page/redirect to login/return to page&rsquo; workflow.</p>

<h3>Read about SimpLabs' experience making Ember.SimpleAuth</h3>

<p>SimpLabs wrote a <a href="http://log.simplabs.com/post/57702291669/better-authentication-in-ember-js">blog post</a> detailing their experiences getting ember authentication to work sensibly and according to the various standards.</p>

<p>They wrapped this functionality up in an ember plugin called <a href="https://github.com/simplabs/ember-simple-auth">Ember.SimpleAuth</a> (and wrote about <a href="http://log.simplabs.com/post/63565686488/ember-simpleauth">how to use it</a>). There&rsquo;s even a <a href="https://github.com/ugisozols/ember-simple-auth-rails-demo">demo rails app</a> that uses it.</p>

<h3>Follow the very detailed ember-auth + devise tutorial</h3>

<p>Someone else has written a rails plugin called <a href="https://github.com/heartsentwined/ember-auth-rails">ember-auth</a> that presumably takes care of both sides (rails server and ember client) of the problem. The true value for learning is in the <a href="https://github.com/heartsentwined/ember-auth-rails-demo/wiki">demo application&rsquo;s tutorial</a> however, which covers:</p>

<ul>
<li>setting up your rails app</li>
<li>setting up devise for rails</li>
<li>modeling the server entities and API endpoints</li>
<li>writing tests for all of the above</li>
<li>setting up your ember app</li>
<li>creating front-end UIs for authentication with ember</li>
</ul>


<p>It also has a separate page that goes into <a href="https://github.com/heartsentwined/ember-auth/wiki/Security">security concerns</a> that highlights a few best practices to keep in mind that frameworks will probably not implement for you:</p>

<ul>
<li>always use https</li>
<li>authentication checks in the client are a convenience only and should never replace auth validation in the server</li>
<li>do not use the &lsquo;current user&rsquo; ID when retrieving priveleged resources (i.e., don&rsquo;t use client-provided data when performing authorization related functionality)</li>
<li>never store any credential information in cookies &ndash; generally just store the server token, which is expendable</li>
<li>do not rely on the client framework alone to clear caches etc when logging out &ndash; ember data for example doesn&rsquo;t offer a way to clear the data store</li>
</ul>


<h3>A few other side-notes while we&rsquo;re talking theoretical</h3>

<p><a href="https://github.com/kristianmandrup/ember-beercan">ember-beercan</a> seems to explore a different approach that I don&rsquo;t really like, but does have some interesting information on using rails and devise on the server side that I might look at later.</p>

<p>A general concern to keep in mind is that for security reasons if the client-side application is not running on the same url/port as the server application the browser might refuse to let them communicate. In this case apparently you should add Rack::Cors to your app (haven&rsquo;t looked into that yet.)</p>

<p>There is also a lot of discussion around whether the client side should be involved at all in the auth negotiation, and perhaps leaving that up to a separate set of pages served by the server, and the server refuse access to the app at all until that time. This makes the client app much simpler as it can always assume that the user is authenticated (see stack overflow questions <a href="http://stackoverflow.com/questions/19401087/ember-js-how-to-get-access-to-store-from-app-object">how to access store from app object</a>, <a href="http://stackoverflow.com/questions/19414393/ember-js-session-cookie-based-authentication-with-rails-and-devise">session cookie based auth with rails and devise</a>.)</p>

<p>When looking into server-side authentication information I found <a href="http://www.robertsosinski.com/2008/02/23/simple-and-restful-authentication-for-ruby-on-rails/">this description of a simple hand-rolled solution</a> to be quite helpful, because it is quite short and covers only the rails side.</p>

<h2>What we want from an auth system</h2>

<p>A minimal authentication system should provide:</p>

<ul>
<li><em>create user</em>: an interface for creating users</li>
<li><em>login user</em>: an interface for authenticating a user based on username or email address and password</li>
<li>error handling: the interface should provide nice handling for errors (if there&rsquo;s an error message it should be printed nicely)</li>
<li>allow session to persist for some amount of time so the user doesn&rsquo;t have to log in every time they come back</li>
</ul>


<p>In addition however, we want the system to provide authrisation:</p>

<ul>
<li>separation of user roles (administrators, regular users, guests/unauthenticated users)</li>
<li>authorisation of access to server resources &ndash; the server should send a <em>401 unauthorized</em> response to unauthorised requests</li>
<li>sensible UI workflow for logging in and unauthorised access attempts

<ul>
<li>when a server responds with a 401 response, the UI should take the user to the login screen</li>
<li>after login, the user should be returned to a sensible page, possibly based on what they were trying to achieve before being redirected to the login page</li>
</ul>
</li>
</ul>


<p>Of course the system has to follow industry practices and where possible use well-established technologies to offload all the dangerous stuff like dealing with passwords and hashing.</p>

<h2>Building an Authenticating app</h2>

<p>I spent a <em>lot</em> of time experimenting with the various options, generally with little success. The executive summary is that most high-level plugins are generally not production ready, or don&rsquo;t work well together with the latest versions of other parts of the architecture. I ended up following a great tutorial on how to hand-roll your own authentication framework in Ember and Rails, using only a single bcrypt Rails gem for the password stuff.</p>

<h3>Failed attempt 1: ember-auth-easy Rails gem</h3>

<p>I was quite hopeful when I found <a href="https://github.com/mharris717/ember-auth-easy">ember-auth-easy</a> which was made to work with <a href="https://github.com/mharris717/ember_auth_rails">ember-auth-rails</a> to provide a full Rails backend/Ember frontend token based authentication solution.</p>

<p>Unfortunately as with so many things in Rails these days, lots of stuff doesn&rsquo;t work with lots of other stuff if you&rsquo;re trying to use the latest versions of things. The devise rails integration was found to be lacking for some reason and there was some mass change involved that broke backwards compatability, and many slightly older gems don&rsquo;t seem to work well anymore. I think it was made to work with Rails 3 as well (I&rsquo;m on Rails 4.)</p>

<p>In addition, there&rsquo;s a whole lot of gems up there and I&rsquo;m starting to get pissed off at having to learn 5 new buzzword-riddled technologies for every one new one I learn.</p>

<p>Before I decided to abandon it I created the following steps (I write it here in case I decide to go back to it later)</p>

<p>```bash Rails Trying to install ember-auth-rails on Rails
vim Gemfile</p>

<h1>add the following: # gem &lsquo;ember_auth_rails&rsquo;, :git => git://github.com/seshbot/ember_auth_rails.git</h1>

<h1>gem &lsquo;ember-data-source&rsquo;, &lsquo;>= 1.0.0.beta.3&rsquo;, &lsquo;&lt; 2.0&rsquo; # ember-data not yet out of beta</h1>

<h1>gem &lsquo;emblem-rails&rsquo;, &lsquo;~> 0.1&rsquo; # easier to write templates</h1>

<h1></h1>

<h1>gem &lsquo;ember-auth-rails&rsquo;, &lsquo;~> 5.0&rsquo; # client-side authentication</h1>

<h1>gem &lsquo;ember-auth-request-jquery-rails&rsquo;, &lsquo;~> 1.0&rsquo; # auth requests via jQuery.ajax</h1>

<h1>gem &lsquo;ember-auth-response-json-rails&rsquo;, &lsquo;~> 1.0&rsquo; # responses in json</h1>

<h1>gem &lsquo;ember-auth-strategy-token-rails&rsquo;, &lsquo;~> 1.0&rsquo; # token authentication</h1>

<h1>gem &lsquo;ember-auth-session-cookie-rails&rsquo;, &lsquo;~> 1.0&rsquo; # use cookies</h1>

<h1>gem &lsquo;ember-auth-module-ember_data-rails&rsquo;, &lsquo;~> 1.0&rsquo; # ember-data integration</h1>

<p>#</p>

<h1>gem &lsquo;devise&rsquo;</h1>

<p>vim config/application.rb</p>

<h1>add &ldquo;require &lsquo;devise&rsquo;&rdquo; to bottom</h1>

<p>bundle install # install new gems
bundle exec rake ember_auth_rails_engine:install:migrations
rails g devise:install
bundle exec rake db:migrate</p>

<p>```</p>

<h3>So I hand-rolled my solution</h3>

<p>I eventually found a great post called <a href="http://coderberry.me/blog/2013/07/08/authentication-with-emberjs-part-1/">authentication with ember.js</a> by a dude who seems to really know what he&rsquo;s talking about. It seems to be a culmination of several weeks of work, and a collaboration with a few other developers. It also includes a link to <a href="https://github.com/rails-api/rails-api">Rails::API</a>, which I really want to use in the future.</p>

<p>The Rails server in the tutorial provides an API that allows creation of a new user, authenticates a user/password combination, and provides admin information to authenticated users. The rails implementation code consists of:</p>

<ul>
<li>router exposes a <em>user</em> resource (for creation and reading) and a <em>session creation</em> post route</li>
<li><code>user</code> model is composed of multiple <code>api-key</code>s &ndash; when the model is loaded, it will create a new api-key if there are no remaining active session keys

<ul>
<li>the <code>user</code> model also invokes the <a href="http://api.rubyonrails.org/classes/ActiveModel/SecurePassword/ClassMethods.html#method-i-has_secure_password">has_secure_password</a> security method that provides an <em>authenticate</em> method (this is built into the Rails framework and uses the bcrypt gem)</li>
</ul>
</li>
<li><code>api-key</code> model is composed of an <em>access token</em> and an <em>exipiry time</em></li>
<li>all controllers have access to an <em>ensure_authenticated_user</em> function that searches for an active API session key that is associated with the current <em>HTTP_AUTHORIZATION</em> HTTP header</li>
<li>the <code>user</code> controller allows an unauthenticated client to show or create a new user record, but requires an authenticated session to show all users</li>
<li>the <code>session</code> controller (invoked via HTTP <em>post</em> from the <em>session creation</em> route) provides <em>login</em> functionality (session creation)

<ul>
<li>first it looks up a <code>user</code> associated with the given username or email</li>
<li>it then invokes the user object&rsquo;s <em>authenticate</em> method with the given password</li>
<li>if either of these steps fail a HTTP 401 response is returned, otherwise a 201 response with the user&rsquo;s <em>session API key</em></li>
</ul>
</li>
</ul>


<p>In the implementation provided in this system, the notion of a <em>session</em> is not actually stored anywhere in the server &ndash; it is merely an access point to a secure token associated with an <code>api-key</code> the user has (a new <code>api-key</code> is created if the session creation request is valid but no current active session key is available.)</p>

<p>An interesting thing I noticed while re-reading the <a href="http://log.simplabs.com/post/57702291669/better-authentication-in-ember-js">SimpLabs blog post</a> I mentioned previously is that this scheme does not allow a user to delete their <em>access token</em> from the server at all. Logging out just means clearing the token cookie &ndash; if the user re-authenticates the server will just dish them out the same token. If I extended the logout action to clear the token from the server however, all of the user&rsquo;s sessions would become invalidated, so I&rsquo;d have to change the scheme to create new access tokens for every separate login. I&rsquo;ll leave it as is for now and have a think about it.</p>

<p>As with all other ember things, the instructions were written with a prior version of ember and so don&rsquo;t work anymore. I had a bit of fun getting it working but a couple of things to keep in mind:</p>

<ul>
<li>use latest version of bcrypt</li>
<li>use &lsquo;jquery-cookie-rails&rsquo; gem</li>
<li>demo uses the <a href="https://github.com/rails-api/rails-api">Rails::API</a> gem instead of creating a whole Rails application with views etc. Because I previously used the <em>ember-rails</em> gem I didn&rsquo;t do this</li>
<li>retrieving data from the store is done differently now &ndash; see the ember docs</li>
<li>accessing the store from anything other than a router can be difficult. I get it with this hackery: <code>var store = GameTableServer.__container__.lookup('store:main')</code> (see <a href="http://stackoverflow.com/questions/19401087/ember-js-how-to-get-access-to-store-from-app-object">http://stackoverflow.com/questions/19401087/ember-js-how-to-get-access-to-store-from-app-object</a>)

<ul>
<li>TRICK! another problem &ndash; store returns an async promise, so you need to wait for that! god dammit. <code>var self = this; store.get(...).then(function(user) { self.set('apiKey', ...)})</code></li>
</ul>
</li>
<li>if something doesn&rsquo;t seem to be working, keep an eye on your browser&rsquo;s javascript console, net traffic and your rails logs (in heroku you can do this with <code>heroku logs</code> or something)</li>
</ul>


<p>You can have a look at the app as it currently stands (as of Jan 2014) on <a href="http://seshbot.herokuapp.com">seshbot.herokuapp.com</a>. If you&rsquo;re reading this much after Jan 2014 however it probably wont be there any longer.</p>

<h3>Alternative: Use &lsquo;devise&rsquo; gem directly</h3>

<p><a href="https://github.com/plataformatec/devise">Devise</a> seems like a very mature and comprehensive rails &lsquo;authentication solution&rsquo; that seems to handle a lot of authentication related problems out of the box in a very configurable way. It takes care of a lot of stuff like sending password reset emails, locking accounts after failed validations, connecting to various auth providers, and lots of other stuff. I will probably end up moving towards it if I ever make an app that warrants that kind of thoroughness.</p>

<p>I found a pretty useful comment on stack overflow that <a href="http://stackoverflow.com/questions/16513066/devise-with-rails-4">details how to get it running quickly with Rails 4</a> &ndash; not sure if thats more useful than the docs.</p>

<p>I started following <a href="https://github.com/heartsentwined/ember-auth-rails-demo/wiki">this very detailed tutorial</a> on using rails + ember + devise and found it very useful. Again, I couldn&rsquo;t get things working well together and had to abandon it. If I were to revisit I might also consider following these <a href="http://avitevet.blogspot.com.es/2012/11/ember-rails-devise-token-authentication.html">very detailed instructions on devise + ember</a>.</p>

<h3>Alternative: Use SimpleAuth for client-side stuff</h3>

<p>SimpLabs' <a href="https://github.com/simplabs/ember-simple-auth">SimpleAuth</a> (discussed in <a href="http://log.simplabs.com/post/63565686488/ember-simpleauth">this SimpLabs blog</a> and mentioned previously in this post) looks pretty cool, and is recently very active. It is still v0.1.0 so I&rsquo;d prefer to keep away from it for now, but if I wanted OAuth integration (to have a &lsquo;login with Facebook&rsquo; or whatever) I might look into using this for the client-side code.</p>

<h2>My current conclusion</h2>

<p>It is laughable that Ember and Rails are in such states of flux that it was easier to hand-roll a solution (although based on a very detailed set of instructions!) than it was to use existing gems and plugins. I hope that changes in the future.</p>

<p>If I ever decide I want a more complex auth solution using rails and ember, I&rsquo;d probably look at using devise for the Rails side and SimpleAuth for the ember side. I would expect a lot of heart-ache along the way though.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating and deploying a Rails + Ember app]]></title>
    <link href="http://seshbot.com/blog/2014/01/15/creating-a-rails-plus-ember-app-from-scratch/"/>
    <updated>2014-01-15T08:54:16+00:00</updated>
    <id>http://seshbot.com/blog/2014/01/15/creating-a-rails-plus-ember-app-from-scratch</id>
    <content type="html"><![CDATA[<p>Today I decided to wield my new Rails and Ember knowledge and&hellip; look into yet another new technology. I thought it would be helpful to have an online app to demonstrate the fruits of my labours, so am deploying a new app to <strong><a href="http://heroku.com">Heroku</a></strong>.</p>

<p>Heroku is an &lsquo;application platform&rsquo; in the cloud, meaning that you can push certain kinds of apps (written in Ruby, Python, Java and Node.js) and it will ensure all the correct infrastructure is in place. When you sign up you get to host one app for free so it&rsquo;s easy to try out.</p>

<p>Later I will probably move to <a href="http://aws.amazon.com">Amazon Web Services</a>, which provides a basic virtual machine in the cloud that you can do anything with. This will allow me to host multiple applications without having to worry about paying money. Heroku <em>does</em> offer some pretty nice scaling, monitoring and deployment tools though (the admin panel literally has a slider to allow you to spin up new application instances.)</p>

<p>This post shows how I went through all steps, including setting up the PostgreSQL database on OSX, creating a skeleton Rails app, and deploying to Heroku. It is a culmination of having gone through several sources:</p>

<ul>
<li>much was taken from this useful step-by-step &lsquo;<a href="http://www.devmynd.com/blog/2013-3-rails-ember-js">Rails + Ember blog post</a>&rsquo; and this <a href="http://www.devmynd.com/blog/2013-10-live-on-the-edge-with-rails-ember-js">follow up post</a> that incorporates changes for newer versions of the frameworks.</li>
<li>when I got to the part involving installing the &lsquo;ember-rails&rsquo; gem, I found that the <a href="https://github.com/emberjs/ember-rails">ember-rails documentation</a> was pretty useful.</li>
<li>some of the Heroku stuff came from the <a href="https://www.codeschool.com/code_tv/heroku">Heroku Code School lesson</a> summary.</li>
</ul>


<!-- more -->


<h2>Choosing a database</h2>

<p>By default Rails will use sqlite3 for its database, and this isn&rsquo;t by default available in Heroku. As I&rsquo;m going to have to do some configuration anyway, I might as well choose a nicer database.</p>

<p>I was deciding between MongoDb and PostgreSQL. MongoDb offers flexibility when it comes to managing file assets in your database, while PostgreSQL is much more well established in existing hosting infrastructures (AWS and Heroku), so can make initial deployment much simpler. Mongo is also more amenable to schema changes because it&rsquo;s a NoSQL schema-less document database, but I think Rails is supposed to make schema changes easy anyway with the various <code>db:</code> commands.</p>

<p>As Heroku comes with PostgreSQL support out of the box, for now I&rsquo;ll go with Postgres. I&rsquo;m as yet not very familiar with Heroku and want to make things easier on myself.</p>

<p>The following installation instructions came from a blog entry on <a href="http://ricochen.wordpress.com/2012/07/20/install-postgres-on-mac-os-x-lion-with-homebrew-howto/">installing PostgreSQL on OSX with HomeBrew</a>:
```bash Installing PostgreSQL on  OSX</p>

<h1>easiest if you have homebrew installed</h1>

<p>brew install postgresql</p>

<h1>ensure it starts up when your machine starts</h1>

<p>ln -sfv /usr/local/opt/postgresql/*.plist ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist
launchctl load ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist</p>

<h1>ensure you don&rsquo;t accidentally run the older version</h1>

<p>echo &lsquo;export PATH=/usr/local/bin:$PATH&rsquo; >> ~/.bash_profile &amp;&amp; . ~/.bash_profile</p>

<h1>create a database user for the application to use</h1>

<h1>(alternatively you should be able to run &lsquo;createuser -d myapp&rsquo;)</h1>

<p>psql postgres <code>whoami</code>
create role myapp with CREATEDB login password &lsquo;password1&rsquo;;
```</p>

<p><em>Notes:</em></p>

<ul>
<li><em>the <code>createuser</code> command can replace the <code>psql</code> command stuff: <code>createuser -d myapp</code></em></li>
<li><em>if the DB username is different to the application name (below) you&rsquo;ll need to change the rails configuration later so it knows which username to use</em></li>
<li><em>I assume Heroku doesn&rsquo;t require you to manage the database at all</em></li>
</ul>


<h2>Creating a simple Rails app</h2>

<p>Creating a Rails app is really simple <em>once you know the commands</em>. So from scratch, if you include all the learning involved behind each command, it&rsquo;s actually not very simple. But these steps make it simple for me.</p>

<p>TIPS: I read somewhere that you should always run <code>bundle exec</code> before running a rails command to ensure that you&rsquo;re only working with gems in your Gemfile. Technically you could run all the commands below without prepending <code>bundle exec</code> however.</p>

<p><code>bash Creating the rails application framework
rails new myapp --database=postgresql
cd myapp
vim config/database.yml # set the database username and password, and on OSX un-comment the 'local' setting
bundle exec rake db:create    # create databases
bundle exec rails generate scaffold Thing name:string # generate model/views/controllers
bundle exec rake db:migrate   # update database with model data
bundle exec rails s           # start rails server localhost:3000
</code></p>

<p><em>Note: I had to uncomment the &lsquo;local&rsquo; setting from my </em>database.yml<em> file because rails couldn&rsquo;t connect due to permission problems on the local socket file. I could have reconfigured postgres instead but meh.</em></p>

<p>OSX users can also use <a href="http://pow.cx/">POW!</a> or <a href="http://anvilformac.com/">Anvil</a> (which uses POW! under the covers) to set up a fake URL pointing to their local rails app directories, so in my case I can visit <a href="http://myapp.dev">http://myapp.dev</a> and it will actually show me the app running on my local machine. It makes the testing cycle a lot quicker.</p>

<p>Add some simple static content:
```bash Generate some simple content in the Rails app
rails generate controller StaticPages home about &mdash;no-test-framework</p>

<h1>set root &lsquo;/&rsquo; route to point to static home page</h1>

<p>vim config/routes.rb # add &ldquo;root &lsquo;static_pages#home&rsquo;&rdquo; beneath other routes
```</p>

<p>Now you should be able to visit <code>localhost:3000</code> and see a generic &lsquo;home&rsquo; page message.</p>

<!-- x_ -->


<h3>Check in to git</h3>

<p>This creates a local git repository, but during the heroku deployment step I&rsquo;ll push it over there too. I&rsquo;ll also push it to GitHub when it looks like more of an app. So in git parlance, this is what I&rsquo;ll have on my local machine:</p>

<p><center><img src='/images/plantuml/7ff3365c0f4bc5f7e2db66419707938a.png'></center></p>

<p><code>bash Create the local 'master' git repository
rake tmp:clear
git init .
git add -f *
git commit -a -m"Initial commit"
</code></p>

<p>I regularly run those <code>git</code> commands to make it easier to revert any mistakes I happen to make.</p>

<h3>Adding ember framework</h3>

<p>I have tried two alternative approaches to creating a new rails app for ember.</p>

<h4>Alternative 1: use the ember &lsquo;edge template&rsquo;</h4>

<p>I think this one is probably the best as it was demonstrated by Yehuda Katz (main Ember guy) in <a href="http://www.youtube.com/watch?v=BpQj9_qEUAc">this live demonstration video</a>. I ran a diff on projects created with and without and it seems to:</p>

<ul>
<li>adds some ember gems to the Gemfile: <code>active_model_serializers</code>, <code>ember-rails</code> and <code>ember-source</code></li>
<li>remove the rails &lsquo;application view layout&rsquo; (<em>app/views/layouts/application.html.erb</em>)</li>
<li>create an ember &lsquo;application template&rsquo; (<em>app/assets/javascripts/templates/application.handlebars</em>)</li>
<li>creates a &lsquo;view asset&rsquo; that generates an index.html with the ember application.js in it (<em>app/views/assets/index.html.erb</em>)</li>
<li>sets up a rails route pointing to the assets controller &lsquo;index&rsquo; action (<em>config/routes.rb</em>)</li>
<li>create empty assets controller and helper files (not sure why)</li>
<li>create a rails ActiveModel &lsquo;application serializer&rsquo; (<em>app/serializers/application_serializer.rb</em>) that does a few things ember requires <!-- x_ --></li>
</ul>


<p>Installing using the edge template is simple. Just replace the <code>rails new</code> step above with the following:</p>

<p>```bash Creating a rails app using the ember template
rails new myapp &mdash;database=postgresql -m <a href="http://emberjs.com/edge_template.rb">http://emberjs.com/edge_template.rb</a>
cd myapp</p>

<h1>edit your database config and Gemfile as before&hellip;</h1>

<p>```</p>

<p><em>Note: I had problems using the remote edge template, so downloaded it and used my local copy instad.</em></p>

<h4>Alternative 2: add ember to an existing rails app</h4>

<p>You could also just add the <code>ember-rails</code> gem directly to your Gemfile, then run <code>rails generate ember:bootstrap</code> and you get a basic Ember framework in your <code>app/assets</code> directory. I also prefer to use javascript directly (as opposed to CoffeeScript, which is the default), so add <code>-g --javascript-engine js</code></p>

<p>```bash Add a simple Ember application framework to the Rails app
vim Gemfile</p>

<h1>add &lsquo;gem &ldquo;ember-rails&rdquo;, github: &ldquo;emberjs/ember-rails&rdquo;&rsquo;</h1>

<p>```</p>

<p>Following this approach I believe you&rsquo;ll have to manually set up the <a href="https://github.com/rails-api/active_model_serializers">Ember ActiveModel Serializer</a> which was written by the Ember guys, and ensures your Ember app understands the format of your Rails app&rsquo;s JSON data. The first alternative does this for you.</p>

<h4>Common to both approaches</h4>

<p>After you have created and updated your Gemfile, you still need to bootstrap the ember environment, and then ensure Ember is running in &lsquo;development&rsquo; mode when Rails is.</p>

<p><em>Note: you can Set &lsquo;developer mode&rsquo; (which enables developer-centric error messages and is apparently quite useful) by updating your </em>config/environments/development.rb<em> with: <code>config.ember.variant = :development</code>. By default running locally will run in dev mode, and running on Heroku will run production mode however.</em></p>

<p>```bash Add a simple Ember application framework to the Rails app
bundle install
bundle exec rails g ember:bootstrap -g &mdash;javascript-engine js
bundle exec rails g ember:install &mdash;head</p>

<p>vim config/environments/development.rb # add config.ember.variant = :development
```</p>

<h2>Deploying to Heroku</h2>

<p>Heroku requires a few rails settings to be modified to work properly:</p>

<p>```bash Change rails app settings for Heroku
vim config/environments/production.rb # heroku runs in prod mode by default</p>

<h1>change &lsquo;config.serve_static_assets&rsquo; to true</h1>

<p>vim Gemfile</p>

<h1>add &ldquo;gem &lsquo;rails_12factor&rsquo;&rdquo;</h1>

<p>```</p>

<p>I have already gone through the Heroku sign up process and installed the toolbelt appropriate for OSX (the toolbelt provides the <code>heroku</code> command line tool), so I won&rsquo;t outline that here.</p>

<p>Installing my rails application on Heroku was then a simple matter of:</p>

<p>```bash Add application in the current directory to Heroku
heroku login
heroku create &mdash;stack cedar
git push heroku master</p>

<h1>whenever you make database changes</h1>

<p>heroku run rake db:migrate</p>

<h1>if you want to push your local database contents to heroku</h1>

<p>heroku db:push # requires the &lsquo;taps&rsquo; gem (&lsquo;gem install taps&rsquo;)
```</p>

<p>This creates an image with a particular configuration of applications and adds a <code>heroku</code> git remote to the git configuration.</p>

<p>Now you can visit the heroku app online (in my case at <a href="http://seshbot.herokuapp.com/">http://seshbot.herokuapp.com/</a>).</p>

<p>TODO: use <code>gem rails_12factor</code>? This alters the rails app a little to make it <a href="http://12factor.net/">12 factor</a>, which is a set of guidelines for how one should build an application to make it easier to administer and deploy. Not really important right now though.</p>

<!-- x_ -->


<h2>Troubleshooting and administering Heroku</h2>

<p><code>bash Various heroku debugging commands
heroku ps    # list running apps
heroku logs  # show application logs
heroku run console # run interactive ruby console
</code></p>

<p><code>bash Various heroku administrative commands
heroku config  # configure remote app through environment variables
heroku apps    # overview of apps
heroku destroy # deallocate remote server
heroku run rake db:migrate
</code></p>

<p><code>heroku config</code> sets environment variables for things you don&rsquo;t want to commit to git (e.g., passwords). Configure your Rails apps to use <code>ENV['MY_VAR']</code> instead of your super secret key, then run <code>heroku config:add MY_VAR=blahblah</code>.</p>

<p>There are also various <code>heroku pg:</code> commands for updating the application database. The application itself doesn&rsquo;t have full admin access to the database so you can&rsquo;t for example write <code>heroku run rake db:drop</code>. Instead you should run <code>heroku pg:reset</code> if you want to clear the database.</p>
]]></content>
  </entry>
  
</feed>
